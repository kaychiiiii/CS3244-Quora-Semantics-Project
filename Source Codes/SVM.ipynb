{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SVM_YS.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"4Kl0ru5OL-LT"},"source":["# Support Vector Machine"]},{"cell_type":"code","metadata":{"id":"_jbJqh_aNBLM","outputId":"29e446f0-2ef5-40b0-ceee-1379d241f61b"},"source":["!pip install six\n","!pip install pandas\n","!pip install numpy\n","!pip install sklearn\n","!pip install matplotlib\n","!pip install imbalanced-learn"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (1.13.0)\n","\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.2.4 is available.\n","You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2021.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.13.0)\n","\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.2.4 is available.\n","You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.1)\n","\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.2.4 is available.\n","You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.24.2)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.1)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (3.0.0)\n","\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.2.4 is available.\n","You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.1.2)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.13.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (44.0.0)\n","\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.2.4 is available.\n","You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (0.8.1)\n","Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (0.24.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.1.0)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn) (1.18.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.24->imbalanced-learn) (3.0.0)\n","\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.2.4 is available.\n","You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"uphFkHMExsR1"},"source":["import pandas as pd\n","import numpy as np\n","import sklearn\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.preprocessing import StandardScaler\n","from imblearn.under_sampling import RandomUnderSampler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jjKud6f_Moan"},"source":["train_set = pd.read_csv('train_set_with_features.csv')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U8dFSkJQNPv1"},"source":["## Data Prep"]},{"cell_type":"code","metadata":{"id":"LJ-QXcssNuVx"},"source":["# Random undersampler to reduce the number of majority class instances to match number of minority class instance.\n","undersample = RandomUnderSampler(sampling_strategy='majority')\n","\n","# Extract only engineered features into x and y\n","x = train_set.drop(['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate', 'Unnamed: 0'], axis=1)\n","y = train_set[['is_duplicate']]\n","\n","# Because gridSearch parameter tuning is slow, only use 50% of model data for training the gridSearch model while searching for best parameters for final SVM model.\n","x_grid_train, x_grid_test, y_grid_train, y_grid_test = train_test_split(x, y, test_size = 0.5, random_state = 42)\n","\n","# Split 80% of data for the final model training and 20% for testing.\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n","\n","# Normalize then undersample data used by final model\n","scaler = StandardScaler()\n","x_train = scaler.fit_transform(x_train)\n","x_test = scaler.transform(x_test)\n","x_train, y_train = undersample.fit_resample(x_train, y_train)\n","\n","# Normalize then undersample data used by gridSearch model\n","x_grid_train = scaler.fit_transform(x_grid_train)\n","x_grid_test = scaler.transform(x_grid_test)\n","x_grid_train, y_grid_train = undersample.fit_resample(x_grid_train, y_grid_train)\n","# gridSearch requires labels to be of a particular shape.\n","y_grid_train = y_grid_train.to_numpy().reshape(-1)\n","y_grid_test = y_grid_test.to_numpy().reshape(-1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sxrpdIG-x5Wo"},"source":["## Parameter tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqrWhzQ_m10X","outputId":"f1ade9ed-0ff5-461e-aa8d-cbbba41ff276"},"source":["# Execute gridSearch to try these parameters for SVM.\n","param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'sigmoid']}\n","grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2, n_jobs=3)\n","grid.fit(x_grid_train ,y_grid_train)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["hi\n","Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"]},{"data":{"text/plain":["GridSearchCV(estimator=SVC(), n_jobs=3,\n","             param_grid={'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001],\n","                         'kernel': ['rbf', 'sigmoid']},\n","             verbose=2)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"18bdgRX4xsR5","outputId":"30f44607-2587-4614-ee55-4565c4a2485e"},"source":["# Best parameters for SVM, but best kernel is not shown\n","print(grid.best_estimator_)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["SVC(C=10, gamma=0.1)\n"]}]},{"cell_type":"code","metadata":{"id":"_pPT09HKxsR6","outputId":"b85fcf84-3820-4e1d-ded1-a58ba956cded"},"source":["# Print out the performance of the SVM model trained by gridSearch using the best parameters.\n","grid_predictions = grid.predict(x_test)\n","print(confusion_matrix(y_test,grid_predictions))\n","print(classification_report(y_test,grid_predictions))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[[32556 18600]\n"," [ 3580 26118]]\n","              precision    recall  f1-score   support\n","\n","           0       0.90      0.64      0.75     51156\n","           1       0.58      0.88      0.70     29698\n","\n","    accuracy                           0.73     80854\n","   macro avg       0.74      0.76      0.72     80854\n","weighted avg       0.78      0.73      0.73     80854\n","\n"]}]},{"cell_type":"code","metadata":{"id":"3GhIijd6Siln"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"REfMtK8UyDbP"},"source":["## Fitting model based on tuned parameters"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":897},"id":"NX1I6ebxNrzJ","outputId":"e4e9468a-df35-49f7-d137-8f3fd426ecb4"},"source":["# Use the parameters found by gridSearch to train the final SVM model with more data (80% instead of 50%).\n","# After trying multiple kernel types since gridSearch did not reveal the best kernel type, 'rbf' is the best.\n","\n","# Kernel = 'rbf'\n","SVM = SVC(C=10, kernel='rbf', degree=3, gamma=0.01)\n","clf = SVM.fit(x_train,y_train)\n","predictions_SVM = SVM.predict(x_test)\n","\n","# Print out the performance of SVM that is trained using the best parameters and \n","print(classification_report(y_test,predictions_SVM))"],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  return f(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.91      0.59      0.71     51156\n","           1       0.56      0.90      0.69     29698\n","\n","    accuracy                           0.70     80854\n","   macro avg       0.74      0.74      0.70     80854\n","weighted avg       0.78      0.70      0.70     80854\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"Nro166PUjR13"},"source":["### Process:\n","1. Normalize feature engineered training data\n","2. Parameter tuning using GridSearchCV which fits the SVM model using several values of each parameter and evaluating it with a 5-fold cross validation. (10000 rows)\n","3. Resulting parameters are C = 100, gamma = 0.01.\n","4. Upon testing, best kernel for those parameters is rbf.\n","\n","Results suggest that the model is better used to predict that a question is NOT a duplicate.\n","\n","\n","### Advantages:\n","1. By using a kernel, there can be separation of the classes even if the data provided is not linearly separable. (https://core.ac.uk/reader/6302770)\n","\n","2. SVM provides good out of sample generalization as it makes use of regularization which helps to prevent overfitting on the dataset. \n","\n","3. SVM can classify data points faster than some other models because it only relies on the support vectors to decide the decision boundary and not all of the data points used to train the model (like kNN).\n","\n","### Disadvantages:\n","1. Does not perform too well with skewed dataset, as in our case. There would be high variance of the decision boundary as the under represented class can skew the decision boundary by a lot. \n","https://www.quora.com/Why-does-SVM-not-perform-well-for-imbalanced-data\n","\n","2. Takes a long time to train the model if the data set is large. \"As you mention, storing the kernel matrix requires memory that scales quadratically with the number of data points. Training time for traditional SVM algorithms also scales superlinearly with the number of data points. So, these algorithms aren't feasible for large data sets.\"\n","https://stats.stackexchange.com/questions/314329/can-support-vector-machine-be-used-in-large-data\n","\n"]}]}