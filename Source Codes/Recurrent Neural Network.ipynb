{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Recurrent Neural Network.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UY8jvidfoVeZ"},"source":["# Recurrent Neural Network (RNN) for NLP\n","\n","---\n","\n","## Outline of Document\n","1. RNN Model with LSTM\n","2. Model Cross Validation\n","3. Evaluation Metrics\n","4. Pros and Cons of Model"]},{"cell_type":"markdown","metadata":{"id":"409Vix3apjR5"},"source":["## RNN Model with LSTM\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HaHVU9mCqKjj"},"source":["### Download Required Libraries"]},{"cell_type":"code","metadata":{"id":"3lEvSQYVpohO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637998920104,"user_tz":-480,"elapsed":12655,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"d53ce19e-b0b0-4270-b2f1-f2482b19f96b"},"source":["!pip install tensorflow\n","!pip install imblearn\n","!pip install keras==2.6.0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in c:\\python38\\lib\\site-packages (2.6.0)\n","Requirement already satisfied: absl-py~=0.10 in c:\\python38\\lib\\site-packages (from tensorflow) (0.14.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\python38\\lib\\site-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: numpy~=1.19.2 in c:\\python38\\lib\\site-packages (from tensorflow) (1.19.5)\n","Requirement already satisfied: six~=1.15.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: gast==0.4.0 in c:\\python38\\lib\\site-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in c:\\python38\\lib\\site-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: tensorflow-estimator~=2.6 in c:\\python38\\lib\\site-packages (from tensorflow) (2.6.0)\n","Requirement already satisfied: tensorboard~=2.6 in c:\\python38\\lib\\site-packages (from tensorflow) (2.6.0)\n","Requirement already satisfied: protobuf>=3.9.2 in c:\\python38\\lib\\site-packages (from tensorflow) (3.18.0)\n","Requirement already satisfied: wheel~=0.35 in c:\\python38\\lib\\site-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: clang~=5.0 in c:\\python38\\lib\\site-packages (from tensorflow) (5.0)\n","Requirement already satisfied: google-pasta~=0.2 in c:\\python38\\lib\\site-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py~=3.1.0 in c:\\python38\\lib\\site-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: astunparse~=1.6.3 in c:\\python38\\lib\\site-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers~=1.12.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.12)\n","Requirement already satisfied: grpcio<2.0,>=1.37.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.41.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in c:\\python38\\lib\\site-packages (from tensorflow) (3.7.4.3)\n","Requirement already satisfied: keras~=2.6 in c:\\python38\\lib\\site-packages (from tensorflow) (2.6.0)\n","Requirement already satisfied: wrapt~=1.12.1 in c:\\python38\\lib\\site-packages (from tensorflow) (1.12.1)\n","Requirement already satisfied: termcolor~=1.1.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.0.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n","Requirement already satisfied: setuptools>=41.0.0 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (41.2.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python38\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\python38\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python38\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python38\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\python38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in c:\\python38\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\n","You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: imblearn in c:\\python38\\lib\\site-packages (0.0)"]},{"output_type":"stream","name":"stderr","text":["WARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\n","You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Requirement already satisfied: imbalanced-learn in c:\\python38\\lib\\site-packages (from imblearn) (0.8.1)\n","Requirement already satisfied: joblib>=0.11 in c:\\python38\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.1)\n","Requirement already satisfied: numpy>=1.13.3 in c:\\python38\\lib\\site-packages (from imbalanced-learn->imblearn) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.24 in c:\\python38\\lib\\site-packages (from imbalanced-learn->imblearn) (0.24.2)\n","Requirement already satisfied: scipy>=0.19.1 in c:\\python38\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python38\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (2.1.0)\n","Requirement already satisfied: keras==2.6.0 in c:\\python38\\lib\\site-packages (2.6.0)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\n","You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"]}]},{"cell_type":"markdown","metadata":{"id":"6VhHS5IuGKP0"},"source":["Import Required Libraries"]},{"cell_type":"code","metadata":{"id":"JEfVCabOEDml"},"source":["import pandas as pd\n","import numpy as np\n","import csv, datetime, time, json\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pickle\n","from zipfile import ZipFile\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model\n","from keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization, LSTM\n","from keras.layers.merge import Concatenate\n","from keras.layers.embeddings import Embedding\n","from keras.utils.vis_utils import plot_model\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.callbacks import Callback, ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from imblearn.under_sampling import RandomUnderSampler\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h3ni2_h5JlHk"},"source":["### Global Constants"]},{"cell_type":"code","metadata":{"id":"tc9yQzTAJkvL"},"source":["MAX_QN_LEN = 219\n","MAX_WORDS = 20000\n","EMBED_DIM = 300 #GloVe is 300dim\n","MAX_SEQUENCE = 25\n","TRAIN_TEST = 0.1\n","VALIDATION = 0.1\n","SEED = 42\n","NUM_EPOCHS = 50\n","DROPOUT = 0.1\n","BATCH_SIZE = 32  #Is the default value\n","REGULARIZER = 0.01\n","OPTIMIZER = 'adam'\n","\n","BASE_MODEL_WEIGHTS_FILE = \"question_pairs_weights.h5\" #Will be saved to a new file if this file didnt exist before\n","MODEL_WEIGHTS_FILE_MLSTM = \"question_pairs_weights_MLSTM.h5\"\n","MODEL_WEIGHTS_FILE_MLSTM_FEATURE = \"question_pairs_weights_MLSTM_w_Features.h5\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nRaxa0S_FSVK"},"source":["### Processing Data"]},{"cell_type":"code","metadata":{"id":"sumGbryLJIay","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637998974974,"user_tz":-480,"elapsed":55,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"b7ca8ad5-784f-46f1-8560-fe1f31207b59"},"source":["%cd \"C:\\Users\\luciu\\Desktop\\NUS Lecture Notes\\Y3S1\\CS3244\\Project\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["C:\\Users\\luciu\\Desktop\\NUS Lecture Notes\\Y3S1\\CS3244\\Project\n"]}]},{"cell_type":"code","metadata":{"id":"xK-xPEyEGHoQ"},"source":["train_set = pd.read_csv('train_set_with_features.csv', index_col=[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":637},"id":"Feqpq_R6Vy55","executionInfo":{"status":"ok","timestamp":1637998981553,"user_tz":-480,"elapsed":83,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"ad7f6d09-2b3b-45cb-a695-2a8597e3e723"},"source":["train_set.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   id  qid1  qid2                                          question1  \\\n","0   0     1     2  what is the step by step guide to invest in sh...   \n","1   1     3     4  what is the story of kohinoor  koh i noor  dia...   \n","2   2     5     6  how can i increase the speed of my internet co...   \n","3   3     7     8  why am i mentally very lonely  how can i solve it   \n","4   4     9    10  which one dissolve in water quikly sugar  salt...   \n","\n","                                           question2  is_duplicate  \\\n","0  what is the step by step guide to invest in sh...             0   \n","1  what would happen if the indian government sto...             0   \n","2  how can internet speed be increased by hacking...             0   \n","3  find the remainder when 23  24   math  is divi...             0   \n","4             which fish would survive in salt water             0   \n","\n","   common_words_ratio  common_tokens_ratio  fuzz_partial_ratio  \\\n","0            0.478261             0.076923                 100   \n","1            0.291667             0.045977                  74   \n","2            0.166667             0.027778                  46   \n","3            0.000000             0.000000                  11   \n","4            0.200000             0.026667                  55   \n","\n","   min_longest_substring  ...  common_stop_words_max  common_nouns_max  \\\n","0               1.000000  ...               0.428571          0.214286   \n","1               0.600000  ...               0.200000          0.200000   \n","2               0.172414  ...               0.142857          0.071429   \n","3               0.040816  ...               0.000000          0.000000   \n","4               0.157895  ...               0.153846          0.153846   \n","\n","   ratio_len_qn  common_word_ratio_max  common_adjectives_max  \\\n","0      1.160714               0.307692                    0.0   \n","1      0.574713               0.172414                    0.0   \n","2      1.241379               0.236111                    0.0   \n","3      0.844828               0.224138                    0.0   \n","4      1.973684               0.213333                    0.0   \n","\n","   fuzz_token_set_ratio  common_words_ratio_min  common_adjectives_min  \\\n","0                   100                1.000000                    0.0   \n","1                    86                0.700000                    0.0   \n","2                    63                0.400000                    0.0   \n","3                    28                0.000000                    0.0   \n","4                    67                0.571429                    0.0   \n","\n","   fuzz_token_sort_ratio  max_longest_substring  \n","0                     93               0.861538  \n","1                     63               0.344828  \n","2                     63               0.138889  \n","3                     25               0.034483  \n","4                     47               0.080000  \n","\n","[5 rows x 27 columns]"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>qid1</th>\n","      <th>qid2</th>\n","      <th>question1</th>\n","      <th>question2</th>\n","      <th>is_duplicate</th>\n","      <th>common_words_ratio</th>\n","      <th>common_tokens_ratio</th>\n","      <th>fuzz_partial_ratio</th>\n","      <th>min_longest_substring</th>\n","      <th>...</th>\n","      <th>common_stop_words_max</th>\n","      <th>common_nouns_max</th>\n","      <th>ratio_len_qn</th>\n","      <th>common_word_ratio_max</th>\n","      <th>common_adjectives_max</th>\n","      <th>fuzz_token_set_ratio</th>\n","      <th>common_words_ratio_min</th>\n","      <th>common_adjectives_min</th>\n","      <th>fuzz_token_sort_ratio</th>\n","      <th>max_longest_substring</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>what is the step by step guide to invest in sh...</td>\n","      <td>what is the step by step guide to invest in sh...</td>\n","      <td>0</td>\n","      <td>0.478261</td>\n","      <td>0.076923</td>\n","      <td>100</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>0.428571</td>\n","      <td>0.214286</td>\n","      <td>1.160714</td>\n","      <td>0.307692</td>\n","      <td>0.0</td>\n","      <td>100</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>93</td>\n","      <td>0.861538</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>what is the story of kohinoor  koh i noor  dia...</td>\n","      <td>what would happen if the indian government sto...</td>\n","      <td>0</td>\n","      <td>0.291667</td>\n","      <td>0.045977</td>\n","      <td>74</td>\n","      <td>0.600000</td>\n","      <td>...</td>\n","      <td>0.200000</td>\n","      <td>0.200000</td>\n","      <td>0.574713</td>\n","      <td>0.172414</td>\n","      <td>0.0</td>\n","      <td>86</td>\n","      <td>0.700000</td>\n","      <td>0.0</td>\n","      <td>63</td>\n","      <td>0.344828</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>how can i increase the speed of my internet co...</td>\n","      <td>how can internet speed be increased by hacking...</td>\n","      <td>0</td>\n","      <td>0.166667</td>\n","      <td>0.027778</td>\n","      <td>46</td>\n","      <td>0.172414</td>\n","      <td>...</td>\n","      <td>0.142857</td>\n","      <td>0.071429</td>\n","      <td>1.241379</td>\n","      <td>0.236111</td>\n","      <td>0.0</td>\n","      <td>63</td>\n","      <td>0.400000</td>\n","      <td>0.0</td>\n","      <td>63</td>\n","      <td>0.138889</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>8</td>\n","      <td>why am i mentally very lonely  how can i solve it</td>\n","      <td>find the remainder when 23  24   math  is divi...</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>11</td>\n","      <td>0.040816</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.844828</td>\n","      <td>0.224138</td>\n","      <td>0.0</td>\n","      <td>28</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>25</td>\n","      <td>0.034483</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>9</td>\n","      <td>10</td>\n","      <td>which one dissolve in water quikly sugar  salt...</td>\n","      <td>which fish would survive in salt water</td>\n","      <td>0</td>\n","      <td>0.200000</td>\n","      <td>0.026667</td>\n","      <td>55</td>\n","      <td>0.157895</td>\n","      <td>...</td>\n","      <td>0.153846</td>\n","      <td>0.153846</td>\n","      <td>1.973684</td>\n","      <td>0.213333</td>\n","      <td>0.0</td>\n","      <td>67</td>\n","      <td>0.571429</td>\n","      <td>0.0</td>\n","      <td>47</td>\n","      <td>0.080000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 27 columns</p>\n","</div>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"t2ee0lPA15x_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637998981968,"user_tz":-480,"elapsed":389,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"0f3b25ba-e84e-4939-cd18-c2a691844573"},"source":["Y_labels = train_set[\"is_duplicate\"]\n","X_features = train_set.drop(\"is_duplicate\", axis=1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_features, Y_labels, test_size=TRAIN_TEST, random_state=SEED)\n","\n","#print(y_train_full.value_counts())\n","\n","#undersample = RandomUnderSampler(sampling_strategy='majority')\n","# undersample = RandomUnderSampler(replacement=True)\n","# X_train, y_train = undersample.fit_resample(X_train_full, y_train_full)\n","\n","print(y_train.value_counts())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0    229467\n","1    134372\n","Name: is_duplicate, dtype: int64\n"]}]},{"cell_type":"code","metadata":{"id":"cSyd_6EbJZn3","colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"status":"ok","timestamp":1637998982129,"user_tz":-480,"elapsed":134,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"d8be3481-beb3-4012-f4d5-91c204161fe4"},"source":["X_train_vector = X_train[['question1', 'question2']]\n","X_train_vector['question2'] = X_train_vector['question2'].astype(str)\n","X_train_vector.head(5)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["C:\\Users\\luciu\\AppData\\Local\\Temp/ipykernel_9692/2052425777.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  X_train_vector['question2'] = X_train_vector['question2'].astype(str)\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                question1  \\\n","305968       what are some examples of two kind of people   \n","166442  how it would be to join time for cat in februa...   \n","87947          what is the best way to sell my restaurant   \n","117082  what will be hillary clinton s foreign policy ...   \n","141242  what food can increase dopamine respectively s...   \n","\n","                                                question2  \n","305968  what are some cool examples of two kinds of pe...  \n","166442   how it would be to join time for cat in february  \n","87947                     how do i sell a restaurant idea  \n","117082  what will hillary clinton s chinese policy be ...  \n","141242  what are the best supplements to increase dopa...  "],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question1</th>\n","      <th>question2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>305968</th>\n","      <td>what are some examples of two kind of people</td>\n","      <td>what are some cool examples of two kinds of pe...</td>\n","    </tr>\n","    <tr>\n","      <th>166442</th>\n","      <td>how it would be to join time for cat in februa...</td>\n","      <td>how it would be to join time for cat in february</td>\n","    </tr>\n","    <tr>\n","      <th>87947</th>\n","      <td>what is the best way to sell my restaurant</td>\n","      <td>how do i sell a restaurant idea</td>\n","    </tr>\n","    <tr>\n","      <th>117082</th>\n","      <td>what will be hillary clinton s foreign policy ...</td>\n","      <td>what will hillary clinton s chinese policy be ...</td>\n","    </tr>\n","    <tr>\n","      <th>141242</th>\n","      <td>what food can increase dopamine respectively s...</td>\n","      <td>what are the best supplements to increase dopa...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"id":"X5p18DBKJfGU","executionInfo":{"status":"ok","timestamp":1637998982436,"user_tz":-480,"elapsed":244,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"d727c22c-6f0c-4195-9af9-b446af508146"},"source":["X_train_features = X_train.drop(['question1', 'question2', 'qid1', 'qid2', 'id'], axis=1)\n","X_train_features.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        common_words_ratio  common_tokens_ratio  fuzz_partial_ratio  \\\n","305968            0.411765             0.060000                  86   \n","166442            0.478261             0.081967                 100   \n","87947             0.125000             0.047619                  63   \n","117082            0.423077             0.051948                  73   \n","141242            0.235294             0.046875                  69   \n","\n","        min_longest_substring  unique_words_count  common_token_ratio_min  \\\n","305968               0.477273                   7                0.068182   \n","166442               1.000000                  11                0.104167   \n","87947                0.354839                   2                0.064516   \n","117082               0.296875                  11                0.062500   \n","141242               0.351852                   4                0.055556   \n","\n","        fuzz_ratio  abs_len_difference  common_stop_words_min  \\\n","305968          94                   6               0.444444   \n","166442          88                  13               0.545455   \n","87947           55                  11               0.000000   \n","117082          79                  13               0.583333   \n","141242          59                  10               0.142857   \n","\n","        common_nouns_min  ...  common_stop_words_max  common_nouns_max  \\\n","305968          0.000000  ...               0.400000          0.000000   \n","166442          0.181818  ...               0.428571          0.142857   \n","87947           0.142857  ...               0.000000          0.111111   \n","117082          0.166667  ...               0.500000          0.142857   \n","141242          0.142857  ...               0.100000          0.100000   \n","\n","        ratio_len_qn  common_word_ratio_max  common_adjectives_max  \\\n","305968      0.880000               0.360000               0.000000   \n","166442      1.270833               0.311475               0.016393   \n","87947       1.354839               0.309524               0.000000   \n","117082      1.203125               0.233766               0.000000   \n","141242      0.843750               0.250000               0.000000   \n","\n","        fuzz_token_set_ratio  common_words_ratio_min  common_adjectives_min  \\\n","305968                    94                0.875000               0.000000   \n","166442                   100                1.000000               0.020833   \n","87947                     65                0.285714               0.000000   \n","117082                    93                0.916667               0.000000   \n","141242                    75                0.571429               0.000000   \n","\n","        fuzz_token_sort_ratio  max_longest_substring  \n","305968                     94               0.420000  \n","166442                     88               0.786885  \n","87947                      52               0.261905  \n","117082                     86               0.246753  \n","141242                     59               0.296875  \n","\n","[5 rows x 21 columns]"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>common_words_ratio</th>\n","      <th>common_tokens_ratio</th>\n","      <th>fuzz_partial_ratio</th>\n","      <th>min_longest_substring</th>\n","      <th>unique_words_count</th>\n","      <th>common_token_ratio_min</th>\n","      <th>fuzz_ratio</th>\n","      <th>abs_len_difference</th>\n","      <th>common_stop_words_min</th>\n","      <th>common_nouns_min</th>\n","      <th>...</th>\n","      <th>common_stop_words_max</th>\n","      <th>common_nouns_max</th>\n","      <th>ratio_len_qn</th>\n","      <th>common_word_ratio_max</th>\n","      <th>common_adjectives_max</th>\n","      <th>fuzz_token_set_ratio</th>\n","      <th>common_words_ratio_min</th>\n","      <th>common_adjectives_min</th>\n","      <th>fuzz_token_sort_ratio</th>\n","      <th>max_longest_substring</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>305968</th>\n","      <td>0.411765</td>\n","      <td>0.060000</td>\n","      <td>86</td>\n","      <td>0.477273</td>\n","      <td>7</td>\n","      <td>0.068182</td>\n","      <td>94</td>\n","      <td>6</td>\n","      <td>0.444444</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.400000</td>\n","      <td>0.000000</td>\n","      <td>0.880000</td>\n","      <td>0.360000</td>\n","      <td>0.000000</td>\n","      <td>94</td>\n","      <td>0.875000</td>\n","      <td>0.000000</td>\n","      <td>94</td>\n","      <td>0.420000</td>\n","    </tr>\n","    <tr>\n","      <th>166442</th>\n","      <td>0.478261</td>\n","      <td>0.081967</td>\n","      <td>100</td>\n","      <td>1.000000</td>\n","      <td>11</td>\n","      <td>0.104167</td>\n","      <td>88</td>\n","      <td>13</td>\n","      <td>0.545455</td>\n","      <td>0.181818</td>\n","      <td>...</td>\n","      <td>0.428571</td>\n","      <td>0.142857</td>\n","      <td>1.270833</td>\n","      <td>0.311475</td>\n","      <td>0.016393</td>\n","      <td>100</td>\n","      <td>1.000000</td>\n","      <td>0.020833</td>\n","      <td>88</td>\n","      <td>0.786885</td>\n","    </tr>\n","    <tr>\n","      <th>87947</th>\n","      <td>0.125000</td>\n","      <td>0.047619</td>\n","      <td>63</td>\n","      <td>0.354839</td>\n","      <td>2</td>\n","      <td>0.064516</td>\n","      <td>55</td>\n","      <td>11</td>\n","      <td>0.000000</td>\n","      <td>0.142857</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.111111</td>\n","      <td>1.354839</td>\n","      <td>0.309524</td>\n","      <td>0.000000</td>\n","      <td>65</td>\n","      <td>0.285714</td>\n","      <td>0.000000</td>\n","      <td>52</td>\n","      <td>0.261905</td>\n","    </tr>\n","    <tr>\n","      <th>117082</th>\n","      <td>0.423077</td>\n","      <td>0.051948</td>\n","      <td>73</td>\n","      <td>0.296875</td>\n","      <td>11</td>\n","      <td>0.062500</td>\n","      <td>79</td>\n","      <td>13</td>\n","      <td>0.583333</td>\n","      <td>0.166667</td>\n","      <td>...</td>\n","      <td>0.500000</td>\n","      <td>0.142857</td>\n","      <td>1.203125</td>\n","      <td>0.233766</td>\n","      <td>0.000000</td>\n","      <td>93</td>\n","      <td>0.916667</td>\n","      <td>0.000000</td>\n","      <td>86</td>\n","      <td>0.246753</td>\n","    </tr>\n","    <tr>\n","      <th>141242</th>\n","      <td>0.235294</td>\n","      <td>0.046875</td>\n","      <td>69</td>\n","      <td>0.351852</td>\n","      <td>4</td>\n","      <td>0.055556</td>\n","      <td>59</td>\n","      <td>10</td>\n","      <td>0.142857</td>\n","      <td>0.142857</td>\n","      <td>...</td>\n","      <td>0.100000</td>\n","      <td>0.100000</td>\n","      <td>0.843750</td>\n","      <td>0.250000</td>\n","      <td>0.000000</td>\n","      <td>75</td>\n","      <td>0.571429</td>\n","      <td>0.000000</td>\n","      <td>59</td>\n","      <td>0.296875</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 21 columns</p>\n","</div>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"XSjBZaAqUn7O","colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"status":"ok","timestamp":1637998982553,"user_tz":-480,"elapsed":96,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"e62aa39a-5f27-4738-ee5d-fc488f0192f3"},"source":["X_test_vector = X_test[['question1', 'question2']]\n","X_test_vector['question2'] = X_test_vector['question2'].astype(str)\n","X_test_vector.head(5)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["C:\\Users\\luciu\\AppData\\Local\\Temp/ipykernel_9692/325267862.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  X_test_vector['question2'] = X_test_vector['question2'].astype(str)\n"]},{"output_type":"execute_result","data":{"text/plain":["                                   question1  \\\n","162455     how good a phil barone saxophones   \n","34708        why have you converted to islam   \n","158538      how do i learn and master things   \n","61781   can dinosaurs exist on other planets   \n","78367      which is the best movie from 2016   \n","\n","                                                question2  \n","162455                    what are phil barone saxophones  \n","34708   why have you converted to islam  what is your ...  \n","158538                    how can i learn mastering music  \n","61781   are there other worlds that have dinosaurs on ...  \n","78367                     which is the best movie of 2016  "],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question1</th>\n","      <th>question2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>162455</th>\n","      <td>how good a phil barone saxophones</td>\n","      <td>what are phil barone saxophones</td>\n","    </tr>\n","    <tr>\n","      <th>34708</th>\n","      <td>why have you converted to islam</td>\n","      <td>why have you converted to islam  what is your ...</td>\n","    </tr>\n","    <tr>\n","      <th>158538</th>\n","      <td>how do i learn and master things</td>\n","      <td>how can i learn mastering music</td>\n","    </tr>\n","    <tr>\n","      <th>61781</th>\n","      <td>can dinosaurs exist on other planets</td>\n","      <td>are there other worlds that have dinosaurs on ...</td>\n","    </tr>\n","    <tr>\n","      <th>78367</th>\n","      <td>which is the best movie from 2016</td>\n","      <td>which is the best movie of 2016</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"id":"lsWj5L_pVsAt","executionInfo":{"status":"ok","timestamp":1637998982649,"user_tz":-480,"elapsed":75,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"e4721b33-853d-449c-ec1e-8908124d70da"},"source":["X_test_features = X_test.drop(['question1', 'question2', 'qid1', 'qid2', 'id'], axis=1)\n","X_test_features.head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        common_words_ratio  common_tokens_ratio  fuzz_partial_ratio  \\\n","162455            0.272727             0.090909                  84   \n","34708             0.375000             0.039216                 100   \n","158538            0.230769             0.031250                  71   \n","61781             0.200000             0.020000                  63   \n","78367             0.428571             0.090909                  90   \n","\n","        min_longest_substring  unique_words_count  common_token_ratio_min  \\\n","162455               0.741935                   3                0.096774   \n","34708                1.000000                   6                0.064516   \n","158538               0.290323                   3                0.032258   \n","61781                0.305556                   3                0.027778   \n","78367                0.774194                   6                0.096774   \n","\n","        fuzz_ratio  abs_len_difference  common_stop_words_min  \\\n","162455          81                   2               0.000000   \n","34708           76                  20               0.666667   \n","158538          73                   1               0.333333   \n","61781           42                  14               0.333333   \n","78367           94                   2               0.428571   \n","\n","        common_nouns_min  ...  common_stop_words_max  common_nouns_max  \\\n","162455          0.000000  ...               0.000000          0.000000   \n","34708           0.000000  ...               0.400000          0.000000   \n","158538          0.000000  ...               0.285714          0.000000   \n","61781           0.000000  ...               0.222222          0.000000   \n","78367           0.142857  ...               0.428571          0.142857   \n","\n","        ratio_len_qn  common_word_ratio_max  common_adjectives_max  \\\n","162455      1.064516               0.424242               0.000000   \n","34708       0.607843               0.352941               0.000000   \n","158538      1.032258               0.437500               0.000000   \n","61781       0.720000               0.260000               0.020000   \n","78367       1.064516               0.515152               0.030303   \n","\n","        fuzz_token_set_ratio  common_words_ratio_min  common_adjectives_min  \\\n","162455                    83                0.600000               0.000000   \n","34708                    100                1.000000               0.000000   \n","158538                    79                0.500000               0.000000   \n","61781                     67                0.500000               0.027778   \n","78367                     95                0.857143               0.032258   \n","\n","        fuzz_token_sort_ratio  max_longest_substring  \n","162455                     75               0.696970  \n","34708                      77               0.607843  \n","158538                     79               0.281250  \n","61781                      58               0.220000  \n","78367                      88               0.727273  \n","\n","[5 rows x 21 columns]"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>common_words_ratio</th>\n","      <th>common_tokens_ratio</th>\n","      <th>fuzz_partial_ratio</th>\n","      <th>min_longest_substring</th>\n","      <th>unique_words_count</th>\n","      <th>common_token_ratio_min</th>\n","      <th>fuzz_ratio</th>\n","      <th>abs_len_difference</th>\n","      <th>common_stop_words_min</th>\n","      <th>common_nouns_min</th>\n","      <th>...</th>\n","      <th>common_stop_words_max</th>\n","      <th>common_nouns_max</th>\n","      <th>ratio_len_qn</th>\n","      <th>common_word_ratio_max</th>\n","      <th>common_adjectives_max</th>\n","      <th>fuzz_token_set_ratio</th>\n","      <th>common_words_ratio_min</th>\n","      <th>common_adjectives_min</th>\n","      <th>fuzz_token_sort_ratio</th>\n","      <th>max_longest_substring</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>162455</th>\n","      <td>0.272727</td>\n","      <td>0.090909</td>\n","      <td>84</td>\n","      <td>0.741935</td>\n","      <td>3</td>\n","      <td>0.096774</td>\n","      <td>81</td>\n","      <td>2</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.064516</td>\n","      <td>0.424242</td>\n","      <td>0.000000</td>\n","      <td>83</td>\n","      <td>0.600000</td>\n","      <td>0.000000</td>\n","      <td>75</td>\n","      <td>0.696970</td>\n","    </tr>\n","    <tr>\n","      <th>34708</th>\n","      <td>0.375000</td>\n","      <td>0.039216</td>\n","      <td>100</td>\n","      <td>1.000000</td>\n","      <td>6</td>\n","      <td>0.064516</td>\n","      <td>76</td>\n","      <td>20</td>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.400000</td>\n","      <td>0.000000</td>\n","      <td>0.607843</td>\n","      <td>0.352941</td>\n","      <td>0.000000</td>\n","      <td>100</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>77</td>\n","      <td>0.607843</td>\n","    </tr>\n","    <tr>\n","      <th>158538</th>\n","      <td>0.230769</td>\n","      <td>0.031250</td>\n","      <td>71</td>\n","      <td>0.290323</td>\n","      <td>3</td>\n","      <td>0.032258</td>\n","      <td>73</td>\n","      <td>1</td>\n","      <td>0.333333</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.285714</td>\n","      <td>0.000000</td>\n","      <td>1.032258</td>\n","      <td>0.437500</td>\n","      <td>0.000000</td>\n","      <td>79</td>\n","      <td>0.500000</td>\n","      <td>0.000000</td>\n","      <td>79</td>\n","      <td>0.281250</td>\n","    </tr>\n","    <tr>\n","      <th>61781</th>\n","      <td>0.200000</td>\n","      <td>0.020000</td>\n","      <td>63</td>\n","      <td>0.305556</td>\n","      <td>3</td>\n","      <td>0.027778</td>\n","      <td>42</td>\n","      <td>14</td>\n","      <td>0.333333</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.222222</td>\n","      <td>0.000000</td>\n","      <td>0.720000</td>\n","      <td>0.260000</td>\n","      <td>0.020000</td>\n","      <td>67</td>\n","      <td>0.500000</td>\n","      <td>0.027778</td>\n","      <td>58</td>\n","      <td>0.220000</td>\n","    </tr>\n","    <tr>\n","      <th>78367</th>\n","      <td>0.428571</td>\n","      <td>0.090909</td>\n","      <td>90</td>\n","      <td>0.774194</td>\n","      <td>6</td>\n","      <td>0.096774</td>\n","      <td>94</td>\n","      <td>2</td>\n","      <td>0.428571</td>\n","      <td>0.142857</td>\n","      <td>...</td>\n","      <td>0.428571</td>\n","      <td>0.142857</td>\n","      <td>1.064516</td>\n","      <td>0.515152</td>\n","      <td>0.030303</td>\n","      <td>95</td>\n","      <td>0.857143</td>\n","      <td>0.032258</td>\n","      <td>88</td>\n","      <td>0.727273</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 21 columns</p>\n","</div>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PuWh9ZRjOZB-","executionInfo":{"status":"ok","timestamp":1637998982717,"user_tz":-480,"elapsed":50,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"ce99be31-95b3-47f1-d69b-92a8d533e6d5"},"source":["X_test_features.columns"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['common_words_ratio', 'common_tokens_ratio', 'fuzz_partial_ratio',\n","       'min_longest_substring', 'unique_words_count', 'common_token_ratio_min',\n","       'fuzz_ratio', 'abs_len_difference', 'common_stop_words_min',\n","       'common_nouns_min', 'mean_len', 'common_stop_words_max',\n","       'common_nouns_max', 'ratio_len_qn', 'common_word_ratio_max',\n","       'common_adjectives_max', 'fuzz_token_set_ratio',\n","       'common_words_ratio_min', 'common_adjectives_min',\n","       'fuzz_token_sort_ratio', 'max_longest_substring'],\n","      dtype='object')"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"D0jzVpCl6D_6"},"source":["Tokenizer creates a keras.tokenizer object which will convert the list of strings that a user pushes in into a list of [tokens]. Based on MAX_NUMBER_OF_WORDS, the tokenizer will cut the number of tokens to that size if needed. In this case, since I put 20k as the limit, we will not be cutting anything. \n","\n","text_to_sequence fits the list of sentences into list of tokens. \n","word_index provides the total number of words in this object"]},{"cell_type":"code","metadata":{"id":"ItdIiD_SNzRr"},"source":["questions = X_train_vector['question1'].tolist() + X_train_vector['question2'].tolist()\n","tokenizer = Tokenizer(num_words=MAX_WORDS)\n","tokenizer.fit_on_texts(questions)\n","#Training Set\n","question1_train = tokenizer.texts_to_sequences(X_train_vector['question1'].tolist())\n","question2_train = tokenizer.texts_to_sequences(X_train_vector['question2'].tolist())\n","\n","#Testing Set\n","question1_test = tokenizer.texts_to_sequences(X_test_vector['question1'].tolist())\n","question2_test = tokenizer.texts_to_sequences(X_test_vector['question2'].tolist())\n","\n","word_index = tokenizer.word_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Waddc5ENOer7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637999035074,"user_tz":-480,"elapsed":57,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"6a4152c1-daff-48b3-825c-9fecdfa2deed"},"source":["print(\"Words in index: %d\" % len(word_index))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Words in index: 82370\n"]}]},{"cell_type":"code","metadata":{"id":"YvZ_v6sJWiIS"},"source":["import pickle\n","\n","# saving\n","with open('tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZnXlxhj640V"},"source":["glove.840b.300d.txt is a 5gb pretrained word vector file by GloVe. The format of the vectors in the txt file should be tuples of word and value. \n","eg. \n","(Dog, 0.12345) (Cat, 0.12000) etc. \n","The code below splits the strings into tuples then forms a dictionary of dict[word] = value for every word in the pretrained model"]},{"cell_type":"code","metadata":{"id":"RJATTdKVJiUe"},"source":["embeddings_index = {}\n","with open('glove.840b.300d.txt', encoding='utf-8') as f:\n","  for line in f:\n","    values = line.split(' ')\n","    word = values[0]\n","    embedding = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = embedding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KnhA9ELTLOKY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637999404102,"user_tz":-480,"elapsed":63,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"64860298-922d-4fa3-ab0d-5868debbe02c"},"source":["print('Word embeddings: %d' % len(embeddings_index))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word embeddings: 2196017\n"]}]},{"cell_type":"markdown","metadata":{"id":"YklmlsBQ7eqt"},"source":["In the function below, The number of words is shrunk to the min of MAX_NUMBER_OF_WORDS vs len(word_index). In this case, the number of words = MAX_NUMBER OF WORDS = 20k\n","\n","For each word in word_index, we take the first 20k words from the our pretrained corpus\n","and try to tag each word from word_index (Which are words from our questions)\n","to the corpus.\n","This seems funny since we will expect to get gaps in the matrix when\n","the word does not map to any vector in the first 20k of the corpus.\n","Hence, when the embedding is done, we print the Null word embeddings (indicating the gap) and we see that it is only 435/20000 which can be accepted.\n","\n","More tuning can be done here to increase/decrease the number of words"]},{"cell_type":"code","metadata":{"id":"iRjaw6clNlO5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637999404206,"user_tz":-480,"elapsed":81,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"e166c577-60c1-49f9-82f0-c16dbc68e754"},"source":["number_of_words = min(MAX_WORDS, len(word_index))\n","word_embedding_matrix = np.zeros((number_of_words + 1, EMBED_DIM))\n","for word, i in word_index.items():\n","  if i > MAX_WORDS:\n","    continue\n","  embedding_vector = embeddings_index.get(word)\n","  if embedding_vector is not None:\n","    word_embedding_matrix[i] = embedding_vector\n","  \n","print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Null word embeddings: 440\n"]}]},{"cell_type":"code","metadata":{"id":"SrclUvACSfcB"},"source":["np.savetxt(\"word_embedding.csv\", word_embedding_matrix, delimiter=\",\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AvqJvMW09H-0"},"source":["For LSTM, the length of our q1 and q2 has to be the same. Therefore, we pad them to an equal length of MAX_SEQUENCE_LENGTH which has been set to 25"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3InRSPvn_lEv","executionInfo":{"status":"ok","timestamp":1637999418367,"user_tz":-480,"elapsed":5554,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"48a6bda9-60ba-4419-8c48-aae8e50f525d"},"source":["q1_train_data = pad_sequences(question1_train, maxlen=MAX_SEQUENCE)\n","q2_train_data = pad_sequences(question2_train, maxlen=MAX_SEQUENCE)\n","\n","q1_test_data =  pad_sequences(question1_test, maxlen=MAX_SEQUENCE)\n","q2_test_data =  pad_sequences(question2_test, maxlen=MAX_SEQUENCE)\n","\n","print('Shape of question1 train data tensor:', q1_train_data.shape)\n","print('Shape of question2 train data tensor:', q2_train_data.shape)\n","\n","print('Shape of question1 test data tensor:', q1_test_data.shape)\n","print('Shape of question2 test data tensor:', q1_test_data.shape)\n","\n","print('Shape of label tensor:', Y_labels.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of question1 train data tensor: (363839, 25)\n","Shape of question2 train data tensor: (363839, 25)\n","Shape of question1 test data tensor: (40427, 25)\n","Shape of question2 test data tensor: (40427, 25)\n","Shape of label tensor: (404266,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"dFPqWAIpWwZ9"},"source":["## Baseline Model"]},{"cell_type":"markdown","metadata":{"id":"724CXImrAUh6"},"source":["#### Training the model"]},{"cell_type":"code","metadata":{"id":"saIRDGWNCZGi"},"source":["question1_train = Input(shape=(MAX_SEQUENCE,)) \n","question2_train = Input(shape=(MAX_SEQUENCE,))  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wk9koFnJNtsn"},"source":["question1_test = Input(shape=(MAX_SEQUENCE,))\n","question2_test = Input(shape=(MAX_SEQUENCE,)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vqhWEP-x9pq4"},"source":["In the two functions below, several Keras functions are used. \n","\n","Embedding(\n","  1. input_dim = (size of vocab list (20k),\n","  2. output_dim = (Length of vector of each word (300),\n","  3. weights = The pretrained matrix that we are using,\n","  4. input_length = Our padded length (25),\n","  5. trainable = False (prevents our pretrained weights from being updated)\n","  )\n","\n","This creates an Embedding object which should take in a sequences of integers [basically list of integers (index of the words)], ensuring that the sequences have the same length. The embedding layer then maps the integer inputs to the vectors found in the embedding matrix in the weights. \n","This creates a 3D matrix of (samples, sequence_length, embedding_dim)\n","\n","Dense(\n","  1. units = Output Dimensions (300)\n","  2. activation = 'relu' refers to rectified linear unit activation function which helps to speed up training and do some stuff (IDK I will find out soon)\n",")\n","\n","This creates an Output object that is shrunk to the matrix size of units, which in this case is EMBEDDING_DIMENSIONS which is 300 (No change)\n","\n","TimeDistributed(\n","1. Layer (which for now is Dense(Embedding))\n","2. q1 (which will be the inputs to feed into this layers for data preparation)\n","\n","TimeDistributed will feed the inputs of q1 or q2 into the layer which prepares each input with the given layer (Dense(Embedding)). In other words. TimeDistributed Layer will apply the same layer to several inputs and each input will produce 1 output. \n","\n","K (keras.backend)\n","1. x is a tensor or variable\n","2. axis = 1 refers to the column of the matrix \n","\n","K is keras backend (imported as K) which provides a list of ultilites such as Max, Min etc.\n","In the lambda function, K.max converts each tensor (matrix) into the column of the maximum x value.\n","(Why is there a need to change to Max of columns though)\n","\n","Lambda\n","1. lambda = The conversion from x to max of x\n","2. output dimensions = expected output shape\n","\n","This is a layer to implement the lambda conversion of x to max of x. (using K.max). \n","\n"]},{"cell_type":"code","metadata":{"id":"xDnCuRx3CepL"},"source":["q1 = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(question1_train) \n","q1 = TimeDistributed(Dense(EMBED_DIM, activation='relu'))(q1) \n","q1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBED_DIM,))(q1) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LGA0PTNfC1ct"},"source":["q2 = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(question2_train)\n","q2 = TimeDistributed(Dense(EMBED_DIM, activation='relu'))(q2)\n","q2 = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBED_DIM,))(q2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AlO6XMGME468"},"source":["Dense (See above)\n","\n","Dropout\n","1. droput rate = 0.1 (DROPOUT Global constant)\n","\n","Dropout Layer randomly sets input units to 0 with a frequency of dropout rate during each step. This is to randomly drop the noise which will help to prevent overfitting. \n","\n","BatchNormalization\n","1. No arguments\n","\n","BatchNormalization applies a transofrmation that maintains the mean output close to 0 and output standard deviation close to 1. This will help to solve internal covariate shift which happens when training our neural network which results in the change in weights of each neuron. The change in weights will cause the distribution of input data for each layer to be different in each iteration which will slow down learning.\n","Normalizing the weights will help to keep the mean output to 0 and standard deviation close to 1 which will reduce covariate shift. \n","\n","\n","Apply 4 layers of training"]},{"cell_type":"code","metadata":{"id":"BwJL8xLxC5l_"},"source":["inputs = concatenate([q1, q2])\n","inputs = Dense(70, activation='relu')(inputs)     #First\n","inputs = Dropout(DROPOUT)(inputs)\n","inputs = BatchNormalization()(inputs)\n","inputs = Dense(150, activation='relu')(inputs)     #Second\n","inputs = Dropout(DROPOUT)(inputs)\n","inputs = BatchNormalization()(inputs)\n","inputs = Dense(70, activation='relu')(inputs)     #Third\n","inputs = Dropout(DROPOUT)(inputs)\n","inputs = BatchNormalization()(inputs)\n","inputs = Dense(35, activation='relu')(inputs)     #Fourth\n","inputs = Dropout(DROPOUT)(inputs)\n","inputs = BatchNormalization()(inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"763A6hvMGu1x"},"source":["Create a compressed output of each tensor as single value output using the Dense layer(1...) which means 1 dimension eg 1 value\n","Sigmoid is used for logistic regression to linearly classify the output. "]},{"cell_type":"code","metadata":{"id":"-Mu6uDJRDx9k"},"source":["final_output = Dense(1, activation='sigmoid')(inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fUQ_crNnD1bQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637999418981,"user_tz":-480,"elapsed":66,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"969a38ca-bc80-4635-f459-763c4f4b0172"},"source":["model = Model(inputs=[question1_train, question2_train], outputs=final_output)\n","model.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 25, 300)      6000300     input_1[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 25, 300)      6000300     input_2[0][0]                    \n","__________________________________________________________________________________________________\n","time_distributed (TimeDistribut (None, 25, 300)      90300       embedding[0][0]                  \n","__________________________________________________________________________________________________\n","time_distributed_1 (TimeDistrib (None, 25, 300)      90300       embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 300)          0           time_distributed[0][0]           \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 300)          0           time_distributed_1[0][0]         \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 600)          0           lambda[0][0]                     \n","                                                                 lambda_1[0][0]                   \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 70)           42070       concatenate[0][0]                \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 70)           0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 70)           280         dropout[0][0]                    \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 150)          10650       batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 150)          0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 150)          600         dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 70)           10570       batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 70)           0           dense_4[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 70)           280         dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 35)           2485        batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 35)           0           dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 35)           140         dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 1)            36          batch_normalization_3[0][0]      \n","==================================================================================================\n","Total params: 12,248,311\n","Trainable params: 247,061\n","Non-trainable params: 12,001,250\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"UspCk6pqHOsf"},"source":["Using the prepared layers, now feed in the training questions and their original labels."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oz_CTLkVEO-h","executionInfo":{"status":"ok","timestamp":1637999528910,"user_tz":-480,"elapsed":109904,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"d16c272d-c10e-4815-bb52-e23a3d3dc2bc"},"source":["print(\"Starting training at\", datetime.datetime.now())\n","t0 = time.time()\n","callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_accuracy', save_best_only=True)]\n","history = model.fit([q1_train_data, q2_train_data], \n","                    y_train, \n","                    epochs=NUM_EPOCHS,\n","                    validation_split=VALIDATION,\n","                    verbose=1,\n","                    batch_size=BATCH_SIZE,\n","                    callbacks=callbacks)\n","\n","t1 = time.time()\n","print(\"Training ended at\", datetime.datetime.now())\n","print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting training at 2021-11-27 15:50:18.348193\n","10233/10233 [==============================] - 109s 10ms/step - loss: 0.5415 - accuracy: 0.7253 - val_loss: 0.4959 - val_accuracy: 0.7527\n","Training ended at 2021-11-27 15:52:08.420747\n","Minutes elapsed: 1.834526\n"]}]},{"cell_type":"code","metadata":{"id":"Pa3cg2xsE1iX"},"source":["max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_accuracy']))\n","print('Maximum validation accuracy = {0:.4f} (epoch {1:d})'.format(max_val_acc, idx+1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BFtcmOSRFJUy"},"source":["#### Testing the model"]},{"cell_type":"code","metadata":{"id":"xUOCtit3FGfd"},"source":["model.load_weights(BASE_MODEL_WEIGHTS_FILE)\n","loss, accuracy = model.evaluate([q1_test_data, q2_test_data], y_test, verbose=0)\n","print('Test loss = {0:.4f}, test accuracy = {1:.4f}'.format(loss, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"emdauvfJps4H"},"source":["####  Evaluation Metrics"]},{"cell_type":"code","metadata":{"id":"ieWGO2GKpvIa"},"source":["print(history.history['accuracy'])\n","print(history.history['loss'])\n","print(history.history['val_accuracy'])\n","print(history.history['val_loss'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"309oswW7HROs"},"source":["plt.plot(history.history['loss'], color ='blue');\n","plt.plot(history.history['val_loss'], color='red');\n","plt.title('Training Loss Vs Validation Loss');\n","plt.xlabel('Epochs');\n","plt.ylabel('Loss');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xtF_uPwYHjMd"},"source":["plt.plot(history.history['accuracy'], color ='blue');\n","plt.plot(history.history['val_accuracy'], color='red');\n","plt.title('Training Accuracy Vs Validation Accuracy');\n","plt.xlabel('Epochs');\n","plt.ylabel('Accuracy');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wxq9KHEuIIJJ"},"source":["incorrects = model.predict([q1_test_data, q2_test_data], verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHLmbPR0IKbP"},"source":["incorrects[incorrects > 0.5] = 1\n","incorrects[incorrects <= 0.5] = 0\n","flattened_incorrect = incorrects.flatten()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOAGhECaIMTO"},"source":["heatmap_data = confusion_matrix(y_test, flattened_incorrect)\n","group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n","group_counts = [\"{0}\".format(value) for value in heatmap_data.flatten()]\n","group_percentages = [\"{0:.2%}\".format(value) for value in heatmap_data.flatten()/np.sum(heatmap_data)]\n","labels = [f\"{group_name}\\n{group_count}\\n{group_percentage}\" for group_name, group_count, group_percentage in zip(group_names,group_counts,group_percentages)]\n","labels = np.asarray(labels).reshape(2,2)\n","\n","sns.heatmap(heatmap_data, annot=labels, fmt=\"\", cmap='Blues')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OpaTqKFLIPDl"},"source":["tn, fp, fn, tp = confusion_matrix(y_test, flattened_incorrect).ravel()\n","Precision = tp / (tp + fp)\n","Accuracy = (tp + tn) / (tp + tn + fp + fn)\n","Recall = tp / (tp + fn)\n","F1 = 2 / ((1/Recall) + (1/Precision))\n","\n","print(\"Precision: \" + str(Precision) + \"\\n\" + \"Accuracy: \" + str(Accuracy) + \"\\n\" + \"Recall: \" + str(Recall) + \"\\n\" + \"F1: \" + str(F1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e0pwDw79rxKN"},"source":["model.save(\"BASELINE_MODEL.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WToCjzMRW0u6"},"source":["## Using Siamnese LSTM with Manhatten Distance\n"]},{"cell_type":"markdown","metadata":{"id":"BTy58zxDbIbO"},"source":["#### Training the Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F6Wmw5BRSU0l","executionInfo":{"status":"ok","timestamp":1637999637217,"user_tz":-480,"elapsed":225,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"5be7189f-65c8-4693-e0a2-62019c0d27be"},"source":["# NEW MODEL TRYING TO USE SIAMNESE LSTM\n","def exponent_neg_manhattan_distance(left, right):\n","    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n","\n","\n","q1_input = Input(shape=(MAX_SEQUENCE,))\n","q2_input = Input(shape=(MAX_SEQUENCE,))\n","\n","LSTM_DIM = 128\n","LSTM_DROPOUT = 0.10\n","LSTM_REGULARIZATION = 0.00\n","\n","q1_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q1_input)\n","q2_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q2_input)\n","\n","manhatten_LSTM = LSTM(LSTM_DIM, kernel_regularizer=l2(LSTM_REGULARIZATION), dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n","q1_LSTM_Output = manhatten_LSTM(q1_LSTM)\n","q2_LSTM_Output = manhatten_LSTM(q2_LSTM)\n","\n","malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([q1_LSTM_Output, q2_LSTM_Output])\n","\n","\n","malstm = Model([q1_input, q2_input], [malstm_distance])\n","malstm.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=['accuracy'])\n","malstm.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_5 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","input_6 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, 25, 300)      6000300     input_5[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_3 (Embedding)         (None, 25, 300)      6000300     input_6[0][0]                    \n","__________________________________________________________________________________________________\n","lstm (LSTM)                     (None, 128)          219648      embedding_2[0][0]                \n","                                                                 embedding_3[0][0]                \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 1)            0           lstm[0][0]                       \n","                                                                 lstm[1][0]                       \n","==================================================================================================\n","Total params: 12,220,248\n","Trainable params: 219,648\n","Non-trainable params: 12,000,600\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"8UhK_Ab8XBQj"},"source":["print(\"Starting training at\", datetime.datetime.now())\n","t0 = time.time()\n","\n","callbacks_MLSTM = [ModelCheckpoint(MODEL_WEIGHTS_FILE_MLSTM, monitor='val_accuracy', save_best_only=True)]\n","malstm_trained = malstm.fit([q1_train_data, q2_train_data],\n","                            y_train,\n","                            batch_size=BATCH_SIZE,\n","                            epochs=NUM_EPOCHS,\n","                            validation_split=VALIDATION,\n","                            verbose = 1,\n","                            callbacks = callbacks_MLSTM)\n","\n","t1 = time.time()\n","print(\"Training ended at\", datetime.datetime.now())\n","print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GgT3ZpWB0l_x"},"source":["max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(malstm_trained.history['val_accuracy']))\n","print('Maximum validation accuracy = {0:.4f} (epoch {1:d})'.format(max_val_acc, idx+1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3CIQJFAzbLBF"},"source":["#### Testing the Model"]},{"cell_type":"code","metadata":{"id":"1ViK64gtbRDs"},"source":["malstm.load_weights(MODEL_WEIGHTS_FILE_MLSTM)\n","loss, accuracy = malstm.evaluate([q1_test_data, q2_test_data], y_test, verbose=1)\n","print('Test loss = {0:.4f}, test accuracy = {1:.4f}'.format(loss, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qw_oZPwUbOMl"},"source":["#### Evaluation Metrics"]},{"cell_type":"code","metadata":{"id":"-8cS03Lfbn4w"},"source":["plt.plot(malstm_trained.history['loss'], color ='blue');\n","plt.plot(malstm_trained.history['val_loss'], color='red');\n","plt.title('Training Loss Vs Validation Loss');\n","plt.xlabel('Epochs');\n","plt.ylabel('Loss');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z4WYZ19Rbqjq"},"source":["plt.plot(malstm_trained.history['accuracy'], color ='blue');\n","plt.plot(malstm_trained.history['val_accuracy'], color='red');\n","plt.title('Training Accuracy Vs Validation Accuracy');\n","plt.xlabel('Epochs');\n","plt.ylabel('Accuracy');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NLkSJzEg5AL_"},"source":["incorrects = malstm.predict([q1_test_data, q2_test_data], verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IPOnl0T9MxcD"},"source":["incorrects[incorrects > 0.5] = 1\n","incorrects[incorrects <= 0.5] = 0\n","flattened_incorrect = incorrects.flatten()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGCUTm3WgEYG"},"source":["heatmap_data = confusion_matrix(y_test, flattened_incorrect)\n","group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n","group_counts = [\"{0}\".format(value) for value in heatmap_data.flatten()]\n","group_percentages = [\"{0:.2%}\".format(value) for value in heatmap_data.flatten()/np.sum(heatmap_data)]\n","labels = [f\"{name}\\n{count}\\n{percentage}\" for name, count, percentage in zip(group_names,group_counts,group_percentages)]\n","labels = np.asarray(labels).reshape(2,2)\n","\n","sns.heatmap(heatmap_data, annot=labels, fmt=\"\", cmap='Blues')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MhoTYhEeg2vR"},"source":["tn, fp, fn, tp = confusion_matrix(y_test, flattened_incorrect).ravel()\n","Precision = tp / (tp + fp)\n","Accuracy = (tp + tn) / (tp + tn + fp + fn)\n","Recall = tp / (tp + fn)\n","F1 = 2 / ((1/Recall) + (1/Precision))\n","\n","print(f\"Precision: {Precision} \\nAccuracy: {Accuracy} \\nRecall: {Recall} \\nF1: {F1}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZoDROVzZ0iJ"},"source":["malstm.save(\"MALTSM_No_Features.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gO60gyuq6DNv"},"source":["## Using Siamnese LSTM with Features"]},{"cell_type":"markdown","metadata":{"id":"skNIW9BA6JOH"},"source":["####Training the Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4a_CeP46H7O","executionInfo":{"status":"ok","timestamp":1638000673313,"user_tz":-480,"elapsed":51,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"4266af7a-3032-49e2-f323-12a6f699fda4"},"source":["def exponent_neg_manhattan_distance(left, right):\n","    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n","\n","\n","q1_input = Input(shape=(MAX_SEQUENCE,))  #25-dim vectors \n","q2_input = Input(shape=(MAX_SEQUENCE,))  #25-dim vectors\n","\n","LSTM_DIM = 128\n","LSTM_DROPOUT = 0.10\n","LSTM_REGULARIZATION = 0.0001\n","\n","q1_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q1_input)  #Creates an Embedding Object and feeds question1 into it\n","q2_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q2_input)\n","\n","\n","manhatten_LSTM = LSTM(LSTM_DIM, kernel_regularizer=l2(LSTM_REGULARIZATION), dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n","q1_LSTM_Output = manhatten_LSTM(q1_LSTM)\n","q2_LSTM_Output = manhatten_LSTM(q2_LSTM)\n","\n","malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([q1_LSTM_Output, q2_LSTM_Output])  #What is x[0][0]??\n","\n","no_of_features = len(X_train_features.columns)\n","feature_input = Input(shape=(no_of_features,))\n","feature_layer = Dense(70, activation='relu')(feature_input)\n","\n","concat_layer = Concatenate()([q1_LSTM_Output, q2_LSTM_Output, feature_layer])\n","concat_layer = Dropout(DROPOUT)(concat_layer)\n","concat_layer = BatchNormalization()(concat_layer)\n","\n","concat_layer = Dense(150, kernel_regularizer=l2(REGULARIZER), activation='relu')(concat_layer)\n","concat_layer = Dropout(DROPOUT)(concat_layer)\n","concat_layer = BatchNormalization()(concat_layer)\n","\n","concat_layer = Dense(70, kernel_regularizer=l2(REGULARIZER), activation='relu')(concat_layer)\n","concat_layer = Dropout(DROPOUT)(concat_layer)\n","concat_layer = BatchNormalization()(concat_layer)\n","\n","concat_layer = Dense(35, kernel_regularizer=l2(REGULARIZER), activation='relu')(concat_layer)\n","concat_layer = Dropout(DROPOUT)(concat_layer)\n","concat_layer = BatchNormalization()(concat_layer)\n","\n","output = Dense(1, activation='sigmoid')(concat_layer)\n","\n","malstm_w_feature = Model([q1_input, q2_input, feature_input], [output])\n","malstm_w_feature.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n","malstm_w_feature.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_7 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","input_8 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","embedding_4 (Embedding)         (None, 25, 300)      6000300     input_7[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_5 (Embedding)         (None, 25, 300)      6000300     input_8[0][0]                    \n","__________________________________________________________________________________________________\n","input_9 (InputLayer)            [(None, 21)]         0                                            \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   (None, 128)          219648      embedding_4[0][0]                \n","                                                                 embedding_5[0][0]                \n","__________________________________________________________________________________________________\n","dense_7 (Dense)                 (None, 70)           1540        input_9[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 326)          0           lstm_1[0][0]                     \n","                                                                 lstm_1[1][0]                     \n","                                                                 dense_7[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 326)          0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 326)          1304        dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","dense_8 (Dense)                 (None, 150)          49050       batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 150)          0           dense_8[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 150)          600         dropout_5[0][0]                  \n","__________________________________________________________________________________________________\n","dense_9 (Dense)                 (None, 70)           10570       batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 70)           0           dense_9[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 70)           280         dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","dense_10 (Dense)                (None, 35)           2485        batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 35)           0           dense_10[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 35)           140         dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","dense_11 (Dense)                (None, 1)            36          batch_normalization_7[0][0]      \n","==================================================================================================\n","Total params: 12,286,253\n","Trainable params: 284,491\n","Non-trainable params: 12,001,762\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"nfbEYhGf6NTx"},"source":["print(\"Starting training at\", datetime.datetime.now())\n","t0 = time.time()\n","\n","callbacks_MLSTM_feature = [ModelCheckpoint(MODEL_WEIGHTS_FILE_MLSTM_FEATURE, monitor='val_accuracy', save_best_only=True)]\n","malstm_w_feature_trained = malstm_w_feature.fit([q1_train_data, q2_train_data, X_train_features],\n","                            y_train,\n","                            batch_size=BATCH_SIZE,\n","                            epochs=NUM_EPOCHS,\n","                            validation_split=VALIDATION,\n","                            verbose = 1,\n","                            callbacks = callbacks_MLSTM_feature)\n","\n","t1 = time.time()\n","print(\"Training ended at\", datetime.datetime.now())\n","print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QwYXoGGC6g5s"},"source":["max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(malstm_w_feature_trained.history['val_accuracy']))\n","print('Maximum validation accuracy = {0:.4f} (epoch {1:d})'.format(max_val_acc, idx+1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K244Bzjg6HXp"},"source":["####Testing the Model"]},{"cell_type":"code","metadata":{"id":"WnXjpQ0J6jOy"},"source":["malstm_w_feature.load_weights(MODEL_WEIGHTS_FILE_MLSTM_FEATURE)\n","loss, accuracy = malstm_w_feature.evaluate([q1_test_data, q2_test_data, X_test_features], y_test, verbose=1)\n","print('Test loss = {0:.4f}, test accuracy = {1:.4f}'.format(loss, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cgPs7ugeSwyO"},"source":["### Evaluation Metrics"]},{"cell_type":"code","metadata":{"id":"xGCbaHkpSsMn"},"source":["incorrects = malstm_w_feature.predict([q1_test_data, q2_test_data, X_test_features], verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t8x8Z1SQSsMo"},"source":["incorrects[incorrects > 0.5] = 1\n","incorrects[incorrects <= 0.5] = 0\n","flattened_incorrect = incorrects.flatten()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fFEhVj8vSsMv"},"source":["heatmap_data = confusion_matrix(y_test, flattened_incorrect)\n","group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n","group_counts = [\"{0}\".format(value) for value in heatmap_data.flatten()]\n","group_percentages = [\"{0:.2%}\".format(value) for value in heatmap_data.flatten()/np.sum(heatmap_data)]\n","labels = [f\"{name}\\n{count}\\n{percentage}\" for name, count, percentage in zip(group_names,group_counts,group_percentages)]\n","labels = np.asarray(labels).reshape(2,2)\n","\n","sns.heatmap(heatmap_data, annot=labels, fmt=\"\", cmap='Blues')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E9-qmtLfSsMw"},"source":["tn, fp, fn, tp = confusion_matrix(y_test, flattened_incorrect).ravel()\n","Precision = tp / (tp + fp)\n","Accuracy = (tp + tn) / (tp + tn + fp + fn)\n","Recall = tp / (tp + fn)\n","F1 = 2 / ((1/Recall) + (1/Precision))\n","\n","print(f\"Precision: {Precision} \\nAccuracy: {Accuracy} \\nRecall: {Recall} \\nF1: {F1}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Pkwz94NWoUJ"},"source":["malstm_w_feature.save(\"MALTSM_w_features.h5\")"],"execution_count":null,"outputs":[]}]}