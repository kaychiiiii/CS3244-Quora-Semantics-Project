{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Recurrent Neural Network Mass Testing.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UY8jvidfoVeZ"},"source":["# Recurrent Neural Network (RNN) for NLP\n","\n","---\n","\n","## Outline of Document\n","1. RNN Model with LSTM\n","2. Model Cross Validation\n","3. Evaluation Metrics"]},{"cell_type":"markdown","metadata":{"id":"409Vix3apjR5"},"source":["## RNN Model with LSTM\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HaHVU9mCqKjj"},"source":["### Download Required Libraries"]},{"cell_type":"code","metadata":{"id":"3lEvSQYVpohO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638001914197,"user_tz":-480,"elapsed":6238,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"7a51daf7-a86b-4812-a48c-e9e33ba08d56"},"source":["!pip install keras\n","!pip install tensorflow\n","!pip install imblearn"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: keras in c:\\python38\\lib\\site-packages (2.6.0)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\n","You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in c:\\python38\\lib\\site-packages (2.6.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.12)\n","Requirement already satisfied: keras~=2.6 in c:\\python38\\lib\\site-packages (from tensorflow) (2.6.0)\n","Requirement already satisfied: tensorboard~=2.6 in c:\\python38\\lib\\site-packages (from tensorflow) (2.6.0)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\python38\\lib\\site-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: absl-py~=0.10 in c:\\python38\\lib\\site-packages (from tensorflow) (0.14.0)\n","Requirement already satisfied: wheel~=0.35 in c:\\python38\\lib\\site-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in c:\\python38\\lib\\site-packages (from tensorflow) (3.7.4.3)\n","Requirement already satisfied: grpcio<2.0,>=1.37.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.41.0)\n","Requirement already satisfied: numpy~=1.19.2 in c:\\python38\\lib\\site-packages (from tensorflow) (1.19.5)\n","Requirement already satisfied: termcolor~=1.1.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: clang~=5.0 in c:\\python38\\lib\\site-packages (from tensorflow) (5.0)\n","Requirement already satisfied: opt-einsum~=3.3.0 in c:\\python38\\lib\\site-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: gast==0.4.0 in c:\\python38\\lib\\site-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: h5py~=3.1.0 in c:\\python38\\lib\\site-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: google-pasta~=0.2 in c:\\python38\\lib\\site-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: tensorflow-estimator~=2.6 in c:\\python38\\lib\\site-packages (from tensorflow) (2.6.0)\n","Requirement already satisfied: six~=1.15.0 in c:\\python38\\lib\\site-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: astunparse~=1.6.3 in c:\\python38\\lib\\site-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: protobuf>=3.9.2 in c:\\python38\\lib\\site-packages (from tensorflow) (3.18.0)\n","Requirement already satisfied: wrapt~=1.12.1 in c:\\python38\\lib\\site-packages (from tensorflow) (1.12.1)\n","Requirement already satisfied: setuptools>=41.0.0 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (41.2.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.0.1)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\python38\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python38\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python38\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\python38\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python38\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\python38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n","Requirement already satisfied: chardet<5,>=3.0.2 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2020.12.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.4)\n","Requirement already satisfied: idna<3,>=2.5 in c:\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in c:\\python38\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING: You are using pip version 21.1.2; however, version 21.3.1 is available.\n","You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: imblearn in c:\\python38\\lib\\site-packages (0.0)\n","Requirement already satisfied: imbalanced-learn in c:\\python38\\lib\\site-packages (from imblearn) (0.8.1)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING: You are using pip version 21.1.2; however, version 21.3.1 is available."]},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn>=0.24 in c:\\python38\\lib\\site-packages (from imbalanced-learn->imblearn) (0.24.2)\n","Requirement already satisfied: scipy>=0.19.1 in c:\\python38\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.3)\n","Requirement already satisfied: joblib>=0.11 in c:\\python38\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.1)\n","Requirement already satisfied: numpy>=1.13.3 in c:\\python38\\lib\\site-packages (from imbalanced-learn->imblearn) (1.19.5)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python38\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (2.1.0)\n"]},{"output_type":"stream","name":"stderr","text":["\n","You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"]}]},{"cell_type":"markdown","metadata":{"id":"6VhHS5IuGKP0"},"source":["Import Required Libraries"]},{"cell_type":"code","metadata":{"id":"JEfVCabOEDml","executionInfo":{"status":"ok","timestamp":1638001914252,"user_tz":-480,"elapsed":27,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}}},"source":["import pandas as pd\n","import numpy as np\n","import csv, datetime, time, json\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pickle\n","from zipfile import ZipFile\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model\n","from keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization, LSTM, Bidirectional\n","from keras.layers.merge import Concatenate\n","from keras.layers.embeddings import Embedding\n","from keras.utils.vis_utils import plot_model\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.callbacks import Callback, ModelCheckpoint\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from imblearn.under_sampling import RandomUnderSampler\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h3ni2_h5JlHk"},"source":["### Global Constants"]},{"cell_type":"code","metadata":{"id":"tc9yQzTAJkvL","executionInfo":{"status":"ok","timestamp":1638012335173,"user_tz":-480,"elapsed":28,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}}},"source":["MAX_QN_LEN = 219\n","MAX_WORDS = 20000\n","EMBED_DIM = 300 #GloVe is 300dim\n","MAX_SEQUENCE = 25\n","TRAIN_TEST = 0.1\n","VALIDATION = 0.1\n","SEED = 42\n","NUM_EPOCHS = 50\n","DROPOUT = 0.1\n","BATCH_SIZE = 32  #Is the default value\n","REGULARIZER = 0.0001\n","OPTIMIZER = 'adam'\n","\n","MODEL_WEIGHTS_FILE_MLSTM = \"question_pairs_weights_MLSTM.h5\"\n","MODEL_WEIGHTS_FILE_MLSTM_COMB = \"question_pairs_weights_MLSTM_COMB.h5\"\n","MODEL_WEIGHTS_FILE_MLSTM_FEATURE = \"question_pairs_weights_MLSTM_w_Features.h5\"\n","MODEL_WEIGHTS_BIDIRECTIONAL_NO_HONEYCOMB = \"question_pairs_weights_MLSTM_BI_NOCOMB.h5\"\n","MODEL_WEIGHTS_BIDIRECTIONAL_HONEYCOMB = \"question_pairs_weights_MLSTM_BI_COMB.h5\""],"execution_count":92,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nRaxa0S_FSVK"},"source":["### Processing Data"]},{"cell_type":"code","metadata":{"id":"sumGbryLJIay","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638001914388,"user_tz":-480,"elapsed":45,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"b287226a-3789-4341-8be0-19be895f416d"},"source":["%cd \"C:\\Users\\luciu\\Desktop\\NUS Lecture Notes\\Y3S1\\CS3244\\Project\""],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["C:\\Users\\luciu\\Desktop\\NUS Lecture Notes\\Y3S1\\CS3244\\Project\n"]}]},{"cell_type":"code","metadata":{"id":"xK-xPEyEGHoQ","executionInfo":{"status":"ok","timestamp":1638001917612,"user_tz":-480,"elapsed":3207,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}}},"source":["train_set = pd.read_csv('features_with_word_embedding.csv', index_col=[0])"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":669},"id":"Feqpq_R6Vy55","executionInfo":{"status":"ok","timestamp":1638001917746,"user_tz":-480,"elapsed":81,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"cfba222d-e81b-4a68-8464-6277b33dd997"},"source":["train_set.head(5)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    qid1  qid2                                          question1  \\\n","id                                                                  \n","0      1     2  what is the step by step guide to invest in sh...   \n","1      3     4  what is the story of kohinoor  koh i noor  dia...   \n","2      5     6  how can i increase the speed of my internet co...   \n","3      7     8  why am i mentally very lonely  how can i solve it   \n","4      9    10  which one dissolve in water quikly sugar  salt...   \n","\n","                                            question2  is_duplicate  \\\n","id                                                                    \n","0   what is the step by step guide to invest in sh...             0   \n","1   what would happen if the indian government sto...             0   \n","2   how can internet speed be increased by hacking...             0   \n","3   find the remainder when 23  24   math  is divi...             0   \n","4              which fish would survive in salt water             0   \n","\n","    common_words_ratio  common_tokens_ratio  fuzz_partial_ratio  \\\n","id                                                                \n","0             0.478261             0.916667                 100   \n","1             0.291667             0.500000                  74   \n","2             0.166667             0.285714                  46   \n","3             0.000000             0.000000                  11   \n","4             0.200000             0.307692                  55   \n","\n","    min_longest_substring  unique_words_count  ...  common_nouns_max  \\\n","id                                             ...                     \n","0                1.000000                  11  ...          0.214286   \n","1                0.600000                   7  ...          0.200000   \n","2                0.172414                   4  ...          0.071429   \n","3                0.040816                   0  ...          0.000000   \n","4                0.157895                   4  ...          0.153846   \n","\n","    ratio_len_qn  common_word_ratio_max  common_adjectives_max  \\\n","id                                                               \n","0       1.160714               0.307692                    0.0   \n","1       0.574713               0.172414                    0.0   \n","2       1.241379               0.236111                    0.0   \n","3       0.844828               0.224138                    0.0   \n","4       1.973684               0.213333                    0.0   \n","\n","    fuzz_token_set_ratio  common_words_ratio_min  common_adjectives_min  \\\n","id                                                                        \n","0                    100                1.000000                    0.0   \n","1                     86                0.700000                    0.0   \n","2                     63                0.400000                    0.0   \n","3                     28                0.000000                    0.0   \n","4                     67                0.571429                    0.0   \n","\n","    fuzz_token_sort_ratio  max_longest_substring  embed_cos_dist  \n","id                                                                \n","0                      93               0.861538        0.031762  \n","1                      63               0.344828        0.266555  \n","2                      63               0.138889        0.118900  \n","3                      25               0.034483        0.443512  \n","4                      47               0.080000        0.244168  \n","\n","[5 rows x 27 columns]"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>qid1</th>\n","      <th>qid2</th>\n","      <th>question1</th>\n","      <th>question2</th>\n","      <th>is_duplicate</th>\n","      <th>common_words_ratio</th>\n","      <th>common_tokens_ratio</th>\n","      <th>fuzz_partial_ratio</th>\n","      <th>min_longest_substring</th>\n","      <th>unique_words_count</th>\n","      <th>...</th>\n","      <th>common_nouns_max</th>\n","      <th>ratio_len_qn</th>\n","      <th>common_word_ratio_max</th>\n","      <th>common_adjectives_max</th>\n","      <th>fuzz_token_set_ratio</th>\n","      <th>common_words_ratio_min</th>\n","      <th>common_adjectives_min</th>\n","      <th>fuzz_token_sort_ratio</th>\n","      <th>max_longest_substring</th>\n","      <th>embed_cos_dist</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>what is the step by step guide to invest in sh...</td>\n","      <td>what is the step by step guide to invest in sh...</td>\n","      <td>0</td>\n","      <td>0.478261</td>\n","      <td>0.916667</td>\n","      <td>100</td>\n","      <td>1.000000</td>\n","      <td>11</td>\n","      <td>...</td>\n","      <td>0.214286</td>\n","      <td>1.160714</td>\n","      <td>0.307692</td>\n","      <td>0.0</td>\n","      <td>100</td>\n","      <td>1.000000</td>\n","      <td>0.0</td>\n","      <td>93</td>\n","      <td>0.861538</td>\n","      <td>0.031762</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>what is the story of kohinoor  koh i noor  dia...</td>\n","      <td>what would happen if the indian government sto...</td>\n","      <td>0</td>\n","      <td>0.291667</td>\n","      <td>0.500000</td>\n","      <td>74</td>\n","      <td>0.600000</td>\n","      <td>7</td>\n","      <td>...</td>\n","      <td>0.200000</td>\n","      <td>0.574713</td>\n","      <td>0.172414</td>\n","      <td>0.0</td>\n","      <td>86</td>\n","      <td>0.700000</td>\n","      <td>0.0</td>\n","      <td>63</td>\n","      <td>0.344828</td>\n","      <td>0.266555</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>how can i increase the speed of my internet co...</td>\n","      <td>how can internet speed be increased by hacking...</td>\n","      <td>0</td>\n","      <td>0.166667</td>\n","      <td>0.285714</td>\n","      <td>46</td>\n","      <td>0.172414</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>0.071429</td>\n","      <td>1.241379</td>\n","      <td>0.236111</td>\n","      <td>0.0</td>\n","      <td>63</td>\n","      <td>0.400000</td>\n","      <td>0.0</td>\n","      <td>63</td>\n","      <td>0.138889</td>\n","      <td>0.118900</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7</td>\n","      <td>8</td>\n","      <td>why am i mentally very lonely  how can i solve it</td>\n","      <td>find the remainder when 23  24   math  is divi...</td>\n","      <td>0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>11</td>\n","      <td>0.040816</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.844828</td>\n","      <td>0.224138</td>\n","      <td>0.0</td>\n","      <td>28</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>25</td>\n","      <td>0.034483</td>\n","      <td>0.443512</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>9</td>\n","      <td>10</td>\n","      <td>which one dissolve in water quikly sugar  salt...</td>\n","      <td>which fish would survive in salt water</td>\n","      <td>0</td>\n","      <td>0.200000</td>\n","      <td>0.307692</td>\n","      <td>55</td>\n","      <td>0.157895</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>0.153846</td>\n","      <td>1.973684</td>\n","      <td>0.213333</td>\n","      <td>0.0</td>\n","      <td>67</td>\n","      <td>0.571429</td>\n","      <td>0.0</td>\n","      <td>47</td>\n","      <td>0.080000</td>\n","      <td>0.244168</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 27 columns</p>\n","</div>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"t2ee0lPA15x_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638001918026,"user_tz":-480,"elapsed":256,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"bd13f4be-6e79-45b9-be84-304ace75fe98"},"source":["Y_labels = train_set[\"is_duplicate\"]\n","X_features = train_set.drop(\"is_duplicate\", axis=1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_features, Y_labels, test_size=TRAIN_TEST, random_state=SEED)\n","\n","#print(y_train_full.value_counts())\n","\n","#undersample = RandomUnderSampler(sampling_strategy='majority')\n","# undersample = RandomUnderSampler(replacement=True)\n","# X_train, y_train = undersample.fit_resample(X_train_full, y_train_full)\n","\n","print(y_train.value_counts())"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["0    229467\n","1    134372\n","Name: is_duplicate, dtype: int64\n"]}]},{"cell_type":"code","metadata":{"id":"cSyd_6EbJZn3","colab":{"base_uri":"https://localhost:8080/","height":342},"executionInfo":{"status":"ok","timestamp":1638001918387,"user_tz":-480,"elapsed":301,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"80874f77-ee03-472d-de19-07a9dbdcbe74"},"source":["X_train_vector = X_train[['question1', 'question2']]\n","X_train_vector['question2'] = X_train_vector['question2'].astype(str)\n","X_train_vector.head(5)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["C:\\Users\\luciu\\AppData\\Local\\Temp/ipykernel_3216/2052425777.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  X_train_vector['question2'] = X_train_vector['question2'].astype(str)\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                question1  \\\n","id                                                          \n","305968       what are some examples of two kind of people   \n","166442  how it would be to join time for cat in februa...   \n","87947          what is the best way to sell my restaurant   \n","117082  what will be hillary clinton s foreign policy ...   \n","141242  what food can increase dopamine respectively s...   \n","\n","                                                question2  \n","id                                                         \n","305968  what are some cool examples of two kinds of pe...  \n","166442   how it would be to join time for cat in february  \n","87947                     how do i sell a restaurant idea  \n","117082  what will hillary clinton s chinese policy be ...  \n","141242  what are the best supplements to increase dopa...  "],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question1</th>\n","      <th>question2</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>305968</th>\n","      <td>what are some examples of two kind of people</td>\n","      <td>what are some cool examples of two kinds of pe...</td>\n","    </tr>\n","    <tr>\n","      <th>166442</th>\n","      <td>how it would be to join time for cat in februa...</td>\n","      <td>how it would be to join time for cat in february</td>\n","    </tr>\n","    <tr>\n","      <th>87947</th>\n","      <td>what is the best way to sell my restaurant</td>\n","      <td>how do i sell a restaurant idea</td>\n","    </tr>\n","    <tr>\n","      <th>117082</th>\n","      <td>what will be hillary clinton s foreign policy ...</td>\n","      <td>what will hillary clinton s chinese policy be ...</td>\n","    </tr>\n","    <tr>\n","      <th>141242</th>\n","      <td>what food can increase dopamine respectively s...</td>\n","      <td>what are the best supplements to increase dopa...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"X5p18DBKJfGU","executionInfo":{"status":"ok","timestamp":1638001918577,"user_tz":-480,"elapsed":145,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"35896c0b-18e2-4803-cd57-17f0b1f7470c"},"source":["X_train_features = X_train.drop(['question1', 'question2', 'qid1', 'qid2'], axis=1)\n","X_train_features.head(5)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        common_words_ratio  common_tokens_ratio  fuzz_partial_ratio  \\\n","id                                                                    \n","305968            0.411765             0.777778                  86   \n","166442            0.478261             0.916667                 100   \n","87947             0.125000             0.222222                  63   \n","117082            0.423077             0.785714                  73   \n","141242            0.235294             0.400000                  69   \n","\n","        min_longest_substring  unique_words_count  common_token_ratio_min  \\\n","id                                                                          \n","305968               0.477273                   7                0.068182   \n","166442               1.000000                  11                0.104167   \n","87947                0.354839                   2                0.064516   \n","117082               0.296875                  11                0.062500   \n","141242               0.351852                   4                0.055556   \n","\n","        fuzz_ratio  abs_len_difference  common_stop_words_min  \\\n","id                                                              \n","305968          94                   6               0.444444   \n","166442          88                  13               0.545455   \n","87947           55                  11               0.000000   \n","117082          79                  13               0.583333   \n","141242          59                  10               0.142857   \n","\n","        common_nouns_min  ...  common_nouns_max  ratio_len_qn  \\\n","id                        ...                                   \n","305968          0.000000  ...          0.000000      0.880000   \n","166442          0.181818  ...          0.142857      1.270833   \n","87947           0.142857  ...          0.111111      1.354839   \n","117082          0.166667  ...          0.142857      1.203125   \n","141242          0.142857  ...          0.100000      0.843750   \n","\n","        common_word_ratio_max  common_adjectives_max  fuzz_token_set_ratio  \\\n","id                                                                           \n","305968               0.360000               0.000000                    94   \n","166442               0.311475               0.016393                   100   \n","87947                0.309524               0.000000                    65   \n","117082               0.233766               0.000000                    93   \n","141242               0.250000               0.000000                    75   \n","\n","        common_words_ratio_min  common_adjectives_min  fuzz_token_sort_ratio  \\\n","id                                                                             \n","305968                0.875000               0.000000                     94   \n","166442                1.000000               0.020833                     88   \n","87947                 0.285714               0.000000                     52   \n","117082                0.916667               0.000000                     86   \n","141242                0.571429               0.000000                     59   \n","\n","        max_longest_substring  embed_cos_dist  \n","id                                             \n","305968               0.420000        0.043271  \n","166442               0.786885        0.025982  \n","87947                0.261905        0.073093  \n","117082               0.246753        0.074425  \n","141242               0.296875        0.085473  \n","\n","[5 rows x 22 columns]"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>common_words_ratio</th>\n","      <th>common_tokens_ratio</th>\n","      <th>fuzz_partial_ratio</th>\n","      <th>min_longest_substring</th>\n","      <th>unique_words_count</th>\n","      <th>common_token_ratio_min</th>\n","      <th>fuzz_ratio</th>\n","      <th>abs_len_difference</th>\n","      <th>common_stop_words_min</th>\n","      <th>common_nouns_min</th>\n","      <th>...</th>\n","      <th>common_nouns_max</th>\n","      <th>ratio_len_qn</th>\n","      <th>common_word_ratio_max</th>\n","      <th>common_adjectives_max</th>\n","      <th>fuzz_token_set_ratio</th>\n","      <th>common_words_ratio_min</th>\n","      <th>common_adjectives_min</th>\n","      <th>fuzz_token_sort_ratio</th>\n","      <th>max_longest_substring</th>\n","      <th>embed_cos_dist</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>305968</th>\n","      <td>0.411765</td>\n","      <td>0.777778</td>\n","      <td>86</td>\n","      <td>0.477273</td>\n","      <td>7</td>\n","      <td>0.068182</td>\n","      <td>94</td>\n","      <td>6</td>\n","      <td>0.444444</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.880000</td>\n","      <td>0.360000</td>\n","      <td>0.000000</td>\n","      <td>94</td>\n","      <td>0.875000</td>\n","      <td>0.000000</td>\n","      <td>94</td>\n","      <td>0.420000</td>\n","      <td>0.043271</td>\n","    </tr>\n","    <tr>\n","      <th>166442</th>\n","      <td>0.478261</td>\n","      <td>0.916667</td>\n","      <td>100</td>\n","      <td>1.000000</td>\n","      <td>11</td>\n","      <td>0.104167</td>\n","      <td>88</td>\n","      <td>13</td>\n","      <td>0.545455</td>\n","      <td>0.181818</td>\n","      <td>...</td>\n","      <td>0.142857</td>\n","      <td>1.270833</td>\n","      <td>0.311475</td>\n","      <td>0.016393</td>\n","      <td>100</td>\n","      <td>1.000000</td>\n","      <td>0.020833</td>\n","      <td>88</td>\n","      <td>0.786885</td>\n","      <td>0.025982</td>\n","    </tr>\n","    <tr>\n","      <th>87947</th>\n","      <td>0.125000</td>\n","      <td>0.222222</td>\n","      <td>63</td>\n","      <td>0.354839</td>\n","      <td>2</td>\n","      <td>0.064516</td>\n","      <td>55</td>\n","      <td>11</td>\n","      <td>0.000000</td>\n","      <td>0.142857</td>\n","      <td>...</td>\n","      <td>0.111111</td>\n","      <td>1.354839</td>\n","      <td>0.309524</td>\n","      <td>0.000000</td>\n","      <td>65</td>\n","      <td>0.285714</td>\n","      <td>0.000000</td>\n","      <td>52</td>\n","      <td>0.261905</td>\n","      <td>0.073093</td>\n","    </tr>\n","    <tr>\n","      <th>117082</th>\n","      <td>0.423077</td>\n","      <td>0.785714</td>\n","      <td>73</td>\n","      <td>0.296875</td>\n","      <td>11</td>\n","      <td>0.062500</td>\n","      <td>79</td>\n","      <td>13</td>\n","      <td>0.583333</td>\n","      <td>0.166667</td>\n","      <td>...</td>\n","      <td>0.142857</td>\n","      <td>1.203125</td>\n","      <td>0.233766</td>\n","      <td>0.000000</td>\n","      <td>93</td>\n","      <td>0.916667</td>\n","      <td>0.000000</td>\n","      <td>86</td>\n","      <td>0.246753</td>\n","      <td>0.074425</td>\n","    </tr>\n","    <tr>\n","      <th>141242</th>\n","      <td>0.235294</td>\n","      <td>0.400000</td>\n","      <td>69</td>\n","      <td>0.351852</td>\n","      <td>4</td>\n","      <td>0.055556</td>\n","      <td>59</td>\n","      <td>10</td>\n","      <td>0.142857</td>\n","      <td>0.142857</td>\n","      <td>...</td>\n","      <td>0.100000</td>\n","      <td>0.843750</td>\n","      <td>0.250000</td>\n","      <td>0.000000</td>\n","      <td>75</td>\n","      <td>0.571429</td>\n","      <td>0.000000</td>\n","      <td>59</td>\n","      <td>0.296875</td>\n","      <td>0.085473</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 22 columns</p>\n","</div>"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"XSjBZaAqUn7O","colab":{"base_uri":"https://localhost:8080/","height":342},"executionInfo":{"status":"ok","timestamp":1638001918792,"user_tz":-480,"elapsed":179,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"c7e547f6-4861-43b1-f20f-da2588ec4fac"},"source":["X_test_vector = X_test[['question1', 'question2']]\n","X_test_vector['question2'] = X_test_vector['question2'].astype(str)\n","X_test_vector.head(5)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["C:\\Users\\luciu\\AppData\\Local\\Temp/ipykernel_3216/325267862.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  X_test_vector['question2'] = X_test_vector['question2'].astype(str)\n"]},{"output_type":"execute_result","data":{"text/plain":["                                   question1  \\\n","id                                             \n","162455     how good a phil barone saxophones   \n","34708        why have you converted to islam   \n","158538      how do i learn and master things   \n","61781   can dinosaurs exist on other planets   \n","78367      which is the best movie from 2016   \n","\n","                                                question2  \n","id                                                         \n","162455                    what are phil barone saxophones  \n","34708   why have you converted to islam  what is your ...  \n","158538                    how can i learn mastering music  \n","61781   are there other worlds that have dinosaurs on ...  \n","78367                     which is the best movie of 2016  "],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question1</th>\n","      <th>question2</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>162455</th>\n","      <td>how good a phil barone saxophones</td>\n","      <td>what are phil barone saxophones</td>\n","    </tr>\n","    <tr>\n","      <th>34708</th>\n","      <td>why have you converted to islam</td>\n","      <td>why have you converted to islam  what is your ...</td>\n","    </tr>\n","    <tr>\n","      <th>158538</th>\n","      <td>how do i learn and master things</td>\n","      <td>how can i learn mastering music</td>\n","    </tr>\n","    <tr>\n","      <th>61781</th>\n","      <td>can dinosaurs exist on other planets</td>\n","      <td>are there other worlds that have dinosaurs on ...</td>\n","    </tr>\n","    <tr>\n","      <th>78367</th>\n","      <td>which is the best movie from 2016</td>\n","      <td>which is the best movie of 2016</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"lsWj5L_pVsAt","executionInfo":{"status":"ok","timestamp":1638001918970,"user_tz":-480,"elapsed":127,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"87d9635a-d119-4e02-8426-cb0b64ebef6d"},"source":["X_test_features = X_test.drop(['question1', 'question2', 'qid1', 'qid2'], axis=1)\n","X_test_features.head(5)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        common_words_ratio  common_tokens_ratio  fuzz_partial_ratio  \\\n","id                                                                    \n","162455            0.272727             0.500000                  84   \n","34708             0.375000             0.600000                 100   \n","158538            0.230769             0.428571                  71   \n","61781             0.200000             0.333333                  63   \n","78367             0.428571             0.857143                  90   \n","\n","        min_longest_substring  unique_words_count  common_token_ratio_min  \\\n","id                                                                          \n","162455               0.741935                   3                0.096774   \n","34708                1.000000                   6                0.064516   \n","158538               0.290323                   3                0.032258   \n","61781                0.305556                   3                0.027778   \n","78367                0.774194                   6                0.096774   \n","\n","        fuzz_ratio  abs_len_difference  common_stop_words_min  \\\n","id                                                              \n","162455          81                   2               0.000000   \n","34708           76                  20               0.666667   \n","158538          73                   1               0.333333   \n","61781           42                  14               0.333333   \n","78367           94                   2               0.428571   \n","\n","        common_nouns_min  ...  common_nouns_max  ratio_len_qn  \\\n","id                        ...                                   \n","162455          0.000000  ...          0.000000      1.064516   \n","34708           0.000000  ...          0.000000      0.607843   \n","158538          0.000000  ...          0.000000      1.032258   \n","61781           0.000000  ...          0.000000      0.720000   \n","78367           0.142857  ...          0.142857      1.064516   \n","\n","        common_word_ratio_max  common_adjectives_max  fuzz_token_set_ratio  \\\n","id                                                                           \n","162455               0.424242               0.000000                    83   \n","34708                0.352941               0.000000                   100   \n","158538               0.437500               0.000000                    79   \n","61781                0.260000               0.020000                    67   \n","78367                0.515152               0.030303                    95   \n","\n","        common_words_ratio_min  common_adjectives_min  fuzz_token_sort_ratio  \\\n","id                                                                             \n","162455                0.600000               0.000000                     75   \n","34708                 1.000000               0.000000                     77   \n","158538                0.500000               0.000000                     79   \n","61781                 0.500000               0.027778                     58   \n","78367                 0.857143               0.032258                     88   \n","\n","        max_longest_substring  embed_cos_dist  \n","id                                             \n","162455               0.696970        0.072947  \n","34708                0.607843        0.100991  \n","158538               0.281250        0.225266  \n","61781                0.220000        0.164511  \n","78367                0.727273        0.000000  \n","\n","[5 rows x 22 columns]"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>common_words_ratio</th>\n","      <th>common_tokens_ratio</th>\n","      <th>fuzz_partial_ratio</th>\n","      <th>min_longest_substring</th>\n","      <th>unique_words_count</th>\n","      <th>common_token_ratio_min</th>\n","      <th>fuzz_ratio</th>\n","      <th>abs_len_difference</th>\n","      <th>common_stop_words_min</th>\n","      <th>common_nouns_min</th>\n","      <th>...</th>\n","      <th>common_nouns_max</th>\n","      <th>ratio_len_qn</th>\n","      <th>common_word_ratio_max</th>\n","      <th>common_adjectives_max</th>\n","      <th>fuzz_token_set_ratio</th>\n","      <th>common_words_ratio_min</th>\n","      <th>common_adjectives_min</th>\n","      <th>fuzz_token_sort_ratio</th>\n","      <th>max_longest_substring</th>\n","      <th>embed_cos_dist</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>162455</th>\n","      <td>0.272727</td>\n","      <td>0.500000</td>\n","      <td>84</td>\n","      <td>0.741935</td>\n","      <td>3</td>\n","      <td>0.096774</td>\n","      <td>81</td>\n","      <td>2</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>1.064516</td>\n","      <td>0.424242</td>\n","      <td>0.000000</td>\n","      <td>83</td>\n","      <td>0.600000</td>\n","      <td>0.000000</td>\n","      <td>75</td>\n","      <td>0.696970</td>\n","      <td>0.072947</td>\n","    </tr>\n","    <tr>\n","      <th>34708</th>\n","      <td>0.375000</td>\n","      <td>0.600000</td>\n","      <td>100</td>\n","      <td>1.000000</td>\n","      <td>6</td>\n","      <td>0.064516</td>\n","      <td>76</td>\n","      <td>20</td>\n","      <td>0.666667</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.607843</td>\n","      <td>0.352941</td>\n","      <td>0.000000</td>\n","      <td>100</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>77</td>\n","      <td>0.607843</td>\n","      <td>0.100991</td>\n","    </tr>\n","    <tr>\n","      <th>158538</th>\n","      <td>0.230769</td>\n","      <td>0.428571</td>\n","      <td>71</td>\n","      <td>0.290323</td>\n","      <td>3</td>\n","      <td>0.032258</td>\n","      <td>73</td>\n","      <td>1</td>\n","      <td>0.333333</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>1.032258</td>\n","      <td>0.437500</td>\n","      <td>0.000000</td>\n","      <td>79</td>\n","      <td>0.500000</td>\n","      <td>0.000000</td>\n","      <td>79</td>\n","      <td>0.281250</td>\n","      <td>0.225266</td>\n","    </tr>\n","    <tr>\n","      <th>61781</th>\n","      <td>0.200000</td>\n","      <td>0.333333</td>\n","      <td>63</td>\n","      <td>0.305556</td>\n","      <td>3</td>\n","      <td>0.027778</td>\n","      <td>42</td>\n","      <td>14</td>\n","      <td>0.333333</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0.720000</td>\n","      <td>0.260000</td>\n","      <td>0.020000</td>\n","      <td>67</td>\n","      <td>0.500000</td>\n","      <td>0.027778</td>\n","      <td>58</td>\n","      <td>0.220000</td>\n","      <td>0.164511</td>\n","    </tr>\n","    <tr>\n","      <th>78367</th>\n","      <td>0.428571</td>\n","      <td>0.857143</td>\n","      <td>90</td>\n","      <td>0.774194</td>\n","      <td>6</td>\n","      <td>0.096774</td>\n","      <td>94</td>\n","      <td>2</td>\n","      <td>0.428571</td>\n","      <td>0.142857</td>\n","      <td>...</td>\n","      <td>0.142857</td>\n","      <td>1.064516</td>\n","      <td>0.515152</td>\n","      <td>0.030303</td>\n","      <td>95</td>\n","      <td>0.857143</td>\n","      <td>0.032258</td>\n","      <td>88</td>\n","      <td>0.727273</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 22 columns</p>\n","</div>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PuWh9ZRjOZB-","executionInfo":{"status":"ok","timestamp":1638001919091,"user_tz":-480,"elapsed":86,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"8ff9207d-fe1e-4fa5-cbc1-e1c21aa34fbc"},"source":["X_test_features.columns"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['common_words_ratio', 'common_tokens_ratio', 'fuzz_partial_ratio',\n","       'min_longest_substring', 'unique_words_count', 'common_token_ratio_min',\n","       'fuzz_ratio', 'abs_len_difference', 'common_stop_words_min',\n","       'common_nouns_min', 'mean_len', 'common_stop_words_max',\n","       'common_nouns_max', 'ratio_len_qn', 'common_word_ratio_max',\n","       'common_adjectives_max', 'fuzz_token_set_ratio',\n","       'common_words_ratio_min', 'common_adjectives_min',\n","       'fuzz_token_sort_ratio', 'max_longest_substring', 'embed_cos_dist'],\n","      dtype='object')"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"D0jzVpCl6D_6"},"source":["Tokenizer creates a keras.tokenizer object which will convert the list of strings that a user pushes in into a list of [tokens]. Based on MAX_NUMBER_OF_WORDS, the tokenizer will cut the number of tokens to that size if needed. In this case, since I put 20k as the limit, we will not be cutting anything. \n","\n","text_to_sequence fits the list of sentences into list of tokens. \n","word_index provides the total number of words in this object"]},{"cell_type":"code","metadata":{"id":"ItdIiD_SNzRr","executionInfo":{"status":"ok","timestamp":1638001941624,"user_tz":-480,"elapsed":22507,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}}},"source":["questions = X_train_vector['question1'].tolist() + X_train_vector['question2'].tolist()\n","tokenizer = Tokenizer(num_words=MAX_WORDS)\n","tokenizer.fit_on_texts(questions)\n","#Training Set\n","question1_train = tokenizer.texts_to_sequences(X_train_vector['question1'].tolist())\n","question2_train = tokenizer.texts_to_sequences(X_train_vector['question2'].tolist())\n","\n","#Testing Set\n","question1_test = tokenizer.texts_to_sequences(X_test_vector['question1'].tolist())\n","question2_test = tokenizer.texts_to_sequences(X_test_vector['question2'].tolist())\n","\n","word_index = tokenizer.word_index"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"Waddc5ENOer7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638001941714,"user_tz":-480,"elapsed":44,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"d2a0741c-8be8-49b0-a0ad-b2436a44bf19"},"source":["print(\"Words in index: %d\" % len(word_index))"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Words in index: 82370\n"]}]},{"cell_type":"code","metadata":{"id":"YvZ_v6sJWiIS","executionInfo":{"status":"ok","timestamp":1638001941758,"user_tz":-480,"elapsed":28,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}}},"source":["import pickle\n","\n","# saving\n","with open('tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DZnXlxhj640V"},"source":["glove.840b.300d.txt is a 5gb pretrained word vector file by GloVe. The format of the vectors in the txt file should be tuples of word and value. \n","eg. \n","(Dog, 0.12345) (Cat, 0.12000) etc. \n","The code below splits the strings into tuples then forms a dictionary of dict[word] = value for every word in the pretrained model"]},{"cell_type":"code","metadata":{"id":"RJATTdKVJiUe","executionInfo":{"status":"ok","timestamp":1638002146125,"user_tz":-480,"elapsed":204331,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}}},"source":["embeddings_index = {}\n","with open('glove.840b.300d.txt', encoding='utf-8') as f:\n","  for line in f:\n","    values = line.split(' ')\n","    word = values[0]\n","    embedding = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = embedding"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"KnhA9ELTLOKY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638002146230,"user_tz":-480,"elapsed":44,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"848d8965-7b1c-44b5-ebeb-f128770ecdbd"},"source":["print('Word embeddings: %d' % len(embeddings_index))"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Word embeddings: 2196017\n"]}]},{"cell_type":"markdown","metadata":{"id":"YklmlsBQ7eqt"},"source":["In the function below, The number of words is shrunk to the min of MAX_NUMBER_OF_WORDS vs len(word_index). In this case, the number of words = MAX_NUMBER OF WORDS = 20k\n","\n","For each word in word_index, we take the first 20k words from the our pretrained corpus\n","and try to tag each word from word_index (Which are words from our questions)\n","to the corpus.\n","This seems funny since we will expect to get gaps in the matrix when\n","the word does not map to any vector in the first 20k of the corpus.\n","Hence, when the embedding is done, we print the Null word embeddings (indicating the gap) and we see that it is only 435/20000 which can be accepted.\n","\n","More tuning can be done here to increase/decrease the number of words"]},{"cell_type":"code","metadata":{"id":"iRjaw6clNlO5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638002146291,"user_tz":-480,"elapsed":45,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"99390c2b-b64d-481c-9df2-9a648d884ba5"},"source":["number_of_words = min(MAX_WORDS, len(word_index))\n","word_embedding_matrix = np.zeros((number_of_words + 1, EMBED_DIM))\n","for word, i in word_index.items():\n","  if i > MAX_WORDS:\n","    continue\n","  embedding_vector = embeddings_index.get(word)\n","  if embedding_vector is not None:\n","    word_embedding_matrix[i] = embedding_vector\n","  \n","print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Null word embeddings: 440\n"]}]},{"cell_type":"markdown","metadata":{"id":"AvqJvMW09H-0"},"source":["For LSTM, the length of our q1 and q2 has to be the same. Therefore, we pad them to an equal length of MAX_SEQUENCE_LENGTH which has been set to 25"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3InRSPvn_lEv","executionInfo":{"status":"ok","timestamp":1638002149557,"user_tz":-480,"elapsed":3234,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"ac392ff9-3836-496b-cad8-f051f3c7f46d"},"source":["q1_train_data = pad_sequences(question1_train, maxlen=MAX_SEQUENCE)\n","q2_train_data = pad_sequences(question2_train, maxlen=MAX_SEQUENCE)\n","\n","q1_test_data =  pad_sequences(question1_test, maxlen=MAX_SEQUENCE)\n","q2_test_data =  pad_sequences(question2_test, maxlen=MAX_SEQUENCE)\n","\n","print('Shape of question1 train data tensor:', q1_train_data.shape)\n","print('Shape of question2 train data tensor:', q2_train_data.shape)\n","\n","print('Shape of question1 test data tensor:', q1_test_data.shape)\n","print('Shape of question2 test data tensor:', q1_test_data.shape)\n","\n","print('Shape of label tensor:', Y_labels.shape)"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of question1 train data tensor: (363839, 25)\n","Shape of question2 train data tensor: (363839, 25)\n","Shape of question1 test data tensor: (40427, 25)\n","Shape of question2 test data tensor: (40427, 25)\n","Shape of label tensor: (404266,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"WToCjzMRW0u6"},"source":["### Jeremy\n","\n","RNN with LSTM without Dense Layers\n"]},{"cell_type":"code","metadata":{"id":"F6Wmw5BRSU0l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638002150010,"user_tz":-480,"elapsed":427,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"a714e880-6d01-42b1-f752-bb1f75c0c048"},"source":["def exponent_neg_manhattan_distance(left, right):\n","    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n","\n","\n","q1_input = Input(shape=(MAX_SEQUENCE,))  #25-dim vectors \n","q2_input = Input(shape=(MAX_SEQUENCE,))  #25-dim vectors\n","\n","LSTM_DIM = 128\n","LSTM_DROPOUT = 0.10\n","LSTM_REGULARIZATION = 0.0001\n","\n","q1_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q1_input)  #Creates an Embedding Object and feeds question1 into it\n","q2_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q2_input)\n","\n","manhatten_LSTM = LSTM(LSTM_DIM, kernel_regularizer=l2(LSTM_REGULARIZATION), dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n","q1_LSTM_Output = manhatten_LSTM(q1_LSTM)\n","q2_LSTM_Output = manhatten_LSTM(q2_LSTM)\n","\n","malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([q1_LSTM_Output, q2_LSTM_Output])  #What is x[0][0]??\n","\n","\n","malstm = Model([q1_input, q2_input], [malstm_distance])\n","malstm.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=['accuracy'])\n","malstm.summary()"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","input_2 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 25, 300)      6000300     input_1[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_1 (Embedding)         (None, 25, 300)      6000300     input_2[0][0]                    \n","__________________________________________________________________________________________________\n","lstm (LSTM)                     (None, 128)          219648      embedding[0][0]                  \n","                                                                 embedding_1[0][0]                \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 1)            0           lstm[0][0]                       \n","                                                                 lstm[1][0]                       \n","==================================================================================================\n","Total params: 12,220,248\n","Trainable params: 219,648\n","Non-trainable params: 12,000,600\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"8UhK_Ab8XBQj"},"source":["print(\"Starting training at\", datetime.datetime.now())\n","t0 = time.time()\n","\n","callbacks_MLSTM = [ModelCheckpoint(MODEL_WEIGHTS_FILE_MLSTM, monitor='val_accuracy', save_best_only=True)]\n","malstm_trained = malstm.fit([q1_train_data, q2_train_data],\n","                            y_train,\n","                            batch_size=BATCH_SIZE,\n","                            epochs=NUM_EPOCHS,\n","                            validation_split=VALIDATION,\n","                            verbose = 1,\n","                            callbacks = callbacks_MLSTM)\n","\n","t1 = time.time()\n","print(\"Training ended at\", datetime.datetime.now())\n","print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZoDROVzZ0iJ","executionInfo":{"status":"ok","timestamp":1638003568977,"user_tz":-480,"elapsed":320,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}}},"source":["malstm.save(\"MALSTM_UNI_NOCOMB.h5\")"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"GgT3ZpWB0l_x"},"source":["max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(malstm_trained.history['val_accuracy']))\n","print('Maximum validation accuracy = {0:.4f} (epoch {1:d})'.format(max_val_acc, idx+1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ViK64gtbRDs"},"source":["malstm.load_weights(MODEL_WEIGHTS_FILE_MLSTM)\n","loss, accuracy = malstm.evaluate([q1_test_data, q2_test_data], y_test, verbose=1)\n","print('Test loss = {0:.4f}, test accuracy = {1:.4f}'.format(loss, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8cS03Lfbn4w"},"source":["plt.plot(malstm_trained.history['loss'], color ='blue');\n","plt.plot(malstm_trained.history['val_loss'], color='red');\n","plt.title('Training Loss Vs Validation Loss');\n","plt.xlabel('Epochs');\n","plt.ylabel('Loss');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z4WYZ19Rbqjq"},"source":["plt.plot(malstm_trained.history['accuracy'], color ='blue');\n","plt.plot(malstm_trained.history['val_accuracy'], color='red');\n","plt.title('Training Accuracy Vs Validation Accuracy');\n","plt.xlabel('Epochs');\n","plt.ylabel('Accuracy');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NLkSJzEg5AL_"},"source":["incorrects = malstm.predict([q1_test_data, q2_test_data], verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IPOnl0T9MxcD","executionInfo":{"status":"ok","timestamp":1638003720439,"user_tz":-480,"elapsed":734,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}}},"source":["incorrects[incorrects > 0.5] = 1\n","incorrects[incorrects <= 0.5] = 0\n","flattened_incorrect = incorrects.flatten()"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"HGCUTm3WgEYG"},"source":["heatmap_data = confusion_matrix(y_test, flattened_incorrect)\n","group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n","group_counts = [\"{0}\".format(value) for value in heatmap_data.flatten()]\n","group_percentages = [\"{0:.2%}\".format(value) for value in heatmap_data.flatten()/np.sum(heatmap_data)]\n","labels = [f\"{group_name}\\n{group_count}\\n{group_percentage}\" for group_name, group_count, group_percentage in zip(group_names,group_counts,group_percentages)]\n","labels = np.asarray(labels).reshape(2,2)\n","\n","sns.heatmap(heatmap_data, annot=labels, fmt=\"\", cmap='Blues')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MhoTYhEeg2vR"},"source":["tn, fp, fn, tp = confusion_matrix(y_test, flattened_incorrect).ravel()\n","Precision = tp / (tp + fp)\n","Accuracy = (tp + tn) / (tp + tn + fp + fn)\n","Recall = tp / (tp + fn)\n","F1 = 2 / ((1/Recall) + (1/Precision))\n","\n","print(f\"Precision: {Precision} \\nAccuracy: {Accuracy} \\nRecall: {Recall} \\nF1: {F1}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1PtqdtYO4Yz9"},"source":["### Jun An\n","Siamese RNN with LSTM, with Dense Layers"]},{"cell_type":"code","metadata":{"id":"SWQvvEtdm-g1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638003724227,"user_tz":-480,"elapsed":833,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"87fbb70c-9746-4844-d61b-aafe272b99f0"},"source":["# NEW MODEL TRYING TO USE SIAMNESE LSTM\n","def exponent_neg_manhattan_distance(left, right):\n","    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n","\n","\n","q1_input = Input(shape=(MAX_SEQUENCE,))  #25-dim vectors \n","q2_input = Input(shape=(MAX_SEQUENCE,))  #25-dim vectors\n","\n","LSTM_DIM = 128\n","LSTM_DROPOUT = 0.10\n","LSTM_REGULARIZATION = 0.0001\n","\n","q1_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q1_input)  #Creates an Embedding Object and feeds question1 into it\n","q2_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q2_input)\n","\n","manhatten_LSTM = LSTM(LSTM_DIM, kernel_regularizer=l2(LSTM_REGULARIZATION), dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n","q1_LSTM_Output = manhatten_LSTM(q1_LSTM)\n","q2_LSTM_Output = manhatten_LSTM(q2_LSTM)\n","\n","concatenated = Concatenate()([q1_LSTM_Output, q2_LSTM_Output])\n","concatenated = Dropout(DROPOUT)(concatenated)\n","concatenated = BatchNormalization()(concatenated)\n","\n","concatenated = Dense(150, kernel_regularizer=l2(REGULARIZER), activation='relu')(concatenated)\n","concatenated = Dropout(DROPOUT)(concatenated)\n","concatenated = BatchNormalization()(concatenated)\n","\n","concatenated = Dense(70, kernel_regularizer=l2(REGULARIZER), activation='relu')(concatenated)\n","concatenated = Dropout(DROPOUT)(concatenated)\n","concatenated = BatchNormalization()(concatenated)\n","\n","concatenated = Dense(35, kernel_regularizer=l2(REGULARIZER), activation='relu')(concatenated)\n","concatenated = Dropout(DROPOUT)(concatenated)\n","concatenated = BatchNormalization()(concatenated)\n","\n","output = Dense(1, activation='sigmoid')(concatenated)\n","\n","malstm_comb = Model([q1_input, q2_input], [output])\n","malstm_comb.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n","malstm_comb.summary()"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_3 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","input_4 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","embedding_2 (Embedding)         (None, 25, 300)      6000300     input_3[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_3 (Embedding)         (None, 25, 300)      6000300     input_4[0][0]                    \n","__________________________________________________________________________________________________\n","lstm_1 (LSTM)                   (None, 128)          219648      embedding_2[0][0]                \n","                                                                 embedding_3[0][0]                \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 256)          0           lstm_1[0][0]                     \n","                                                                 lstm_1[1][0]                     \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 256)          0           concatenate[0][0]                \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 256)          1024        dropout[0][0]                    \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 150)          38550       batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 150)          0           dense[0][0]                      \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 150)          600         dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 70)           10570       batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 70)           0           dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 70)           280         dropout_2[0][0]                  \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 35)           2485        batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 35)           0           dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 35)           140         dropout_3[0][0]                  \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 1)            36          batch_normalization_3[0][0]      \n","==================================================================================================\n","Total params: 12,273,933\n","Trainable params: 272,311\n","Non-trainable params: 12,001,622\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"7NBItAK8qGfz"},"source":["print(\"Starting training at\", datetime.datetime.now())\n","t0 = time.time()\n","\n","callbacks_MLSTM_Comb = [ModelCheckpoint(MODEL_WEIGHTS_FILE_MLSTM_COMB, monitor='val_accuracy', save_best_only=True)]\n","malstm_comb_trained = malstm_comb.fit([q1_train_data, q2_train_data],\n","                            y_train,\n","                            batch_size=BATCH_SIZE,\n","                            epochs=NUM_EPOCHS,\n","                            validation_split=VALIDATION,\n","                            verbose = 1,\n","                            callbacks = callbacks_MLSTM_Comb)\n","\n","t1 = time.time()\n","print(\"Training ended at\", datetime.datetime.now())\n","print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_VXVDDos6PM"},"source":["malstm_comb.save(\"MALSTM_UNI_COMB\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sum_4sKSqrss"},"source":["max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(malstm_comb_trained.history['val_accuracy']))\n","print('Maximum validation accuracy = {0:.4f} (epoch {1:d})'.format(max_val_acc, idx+1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2mWGvAeBqrst"},"source":["malstm_comb.load_weights(MODEL_WEIGHTS_FILE_MLSTM_COMB)\n","loss, accuracy = malstm_comb.evaluate([q1_test_data, q2_test_data], y_test, verbose=1)\n","print('Test loss = {0:.4f}, test accuracy = {1:.4f}'.format(loss, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VFKBoOczqrst"},"source":["plt.plot(malstm_trained.history['loss'], color ='blue');\n","plt.plot(malstm_trained.history['val_loss'], color='red');\n","plt.title('Training Loss Vs Validation Loss');\n","plt.xlabel('Epochs');\n","plt.ylabel('Loss');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g0em9JaIqrst"},"source":["plt.plot(malstm_trained.history['accuracy'], color ='blue');\n","plt.plot(malstm_trained.history['val_accuracy'], color='red');\n","plt.title('Training Accuracy Vs Validation Accuracy');\n","plt.xlabel('Epochs');\n","plt.ylabel('Accuracy');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1gh2WbzUqrst"},"source":["incorrects = malstm_comb.predict([q1_test_data, q2_test_data], verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ucBTgQcJqrst","executionInfo":{"status":"ok","timestamp":1638004807985,"user_tz":-480,"elapsed":190,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}}},"source":["incorrects[incorrects > 0.5] = 1\n","incorrects[incorrects <= 0.5] = 0\n","flattened_incorrect = incorrects.flatten()"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUBuDcNjqrst"},"source":["heatmap_data = confusion_matrix(y_test, flattened_incorrect)\n","group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n","group_counts = [\"{0}\".format(value) for value in heatmap_data.flatten()]\n","group_percentages = [\"{0:.2%}\".format(value) for value in heatmap_data.flatten()/np.sum(heatmap_data)]\n","labels = [f\"{group_name}\\n{group_count}\\n{group_percentage}\" for group_name, group_count, group_percentage in zip(group_names,group_counts,group_percentages)]\n","labels = np.asarray(labels).reshape(2,2)\n","\n","sns.heatmap(heatmap_data, annot=labels, fmt=\"\", cmap='Blues')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"81RNMQN8qrst"},"source":["tn, fp, fn, tp = confusion_matrix(y_test, flattened_incorrect).ravel()\n","Precision = tp / (tp + fp)\n","Accuracy = (tp + tn) / (tp + tn + fp + fn)\n","Recall = tp / (tp + fn)\n","F1 = 2 / ((1/Recall) + (1/Precision))\n","\n","print(f\"Precision: {Precision} \\nAccuracy: {Accuracy} \\nRecall: {Recall} \\nF1: {F1}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hT__l5fnkK7G"},"source":["### Kay Chi\n","\n","Siamese Neural Network Bidirectional LSTM, no dense layers"]},{"cell_type":"code","metadata":{"id":"O2TdPTvfkOLd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638004809650,"user_tz":-480,"elapsed":298,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"7c969d76-4a6a-4096-e72c-efd3ba8967d8"},"source":["def exponent_neg_manhattan_distance(left, right):\n","    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n","\n","\n","q1_input = Input(shape=(MAX_SEQUENCE,))  #25-dim vectors \n","q2_input = Input(shape=(MAX_SEQUENCE,))  #25-dim vectors\n","\n","LSTM_DIM = 128\n","LSTM_DROPOUT = 0.10\n","LSTM_REGULARIZATION = 0.00\n","\n","q1_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q1_input)  #Creates an Embedding Object and feeds question1 into it\n","q2_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q2_input)\n","\n","manhatten_LSTM = Bidirectional(LSTM(LSTM_DIM, kernel_regularizer=l2(LSTM_REGULARIZATION), dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT))\n","q1_LSTM_Output = manhatten_LSTM(q1_LSTM)\n","q2_LSTM_Output = manhatten_LSTM(q2_LSTM)\n","\n","malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([q1_LSTM_Output, q2_LSTM_Output])  #What is x[0][0]??\n","\n","malstm_bi_no_comb = Model([q1_input, q2_input], [malstm_distance])\n","malstm_bi_no_comb.compile(loss='mean_squared_error', optimizer=OPTIMIZER, metrics=['accuracy'])\n","malstm_bi_no_comb.summary()"],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_5 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","input_6 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","embedding_4 (Embedding)         (None, 25, 300)      6000300     input_5[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_5 (Embedding)         (None, 25, 300)      6000300     input_6[0][0]                    \n","__________________________________________________________________________________________________\n","bidirectional (Bidirectional)   (None, 256)          439296      embedding_4[0][0]                \n","                                                                 embedding_5[0][0]                \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 1)            0           bidirectional[0][0]              \n","                                                                 bidirectional[1][0]              \n","==================================================================================================\n","Total params: 12,439,896\n","Trainable params: 439,296\n","Non-trainable params: 12,000,600\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"WJnAAiFtk-P4"},"source":["print(\"Starting training at\", datetime.datetime.now())\n","t0 = time.time()\n","\n","callbacks_MLSTM = [ModelCheckpoint(MODEL_WEIGHTS_BIDIRECTIONAL_NO_HONEYCOMB, monitor='val_accuracy', save_best_only=True)]\n","malstm_bi_no_comb_trained = malstm_bi_no_comb.fit([q1_train_data, q2_train_data],\n","                            y_train,\n","                            batch_size=BATCH_SIZE,\n","                            epochs=NUM_EPOCHS,\n","                            validation_split=VALIDATION,\n","                            verbose = 1,\n","                            callbacks = callbacks_MLSTM)\n","\n","t1 = time.time()\n","print(\"Training ended at\", datetime.datetime.now())\n","print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5X4jM7WjlonL"},"source":["malstm_bi_no_comb.save(\"MALSTM_BI_NO_COMB\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N1-hhq-hlvJ-"},"source":["max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(malstm_bi_no_comb_trained.history['val_accuracy']))\n","print('Maximum validation accuracy = {0:.4f} (epoch {1:d})'.format(max_val_acc, idx+1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ton5oP-5lxRM"},"source":["malstm_bi_no_comb.load_weights(MODEL_WEIGHTS_BIDIRECTIONAL_NO_HONEYCOMB)\n","loss, accuracy = malstm_bi_no_comb.evaluate([q1_test_data, q2_test_data], y_test, verbose=1)\n","print('Test loss = {0:.4f}, test accuracy = {1:.4f}'.format(loss, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbzIo67mly96"},"source":["plt.plot(malstm_bi_no_comb_trained.history['loss'], color ='blue');\n","plt.plot(malstm_bi_no_comb_trained.history['val_loss'], color='red');\n","plt.title('Training Loss Vs Validation Loss');\n","plt.xlabel('Epochs');\n","plt.ylabel('Loss');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xkoDgX6Il5u7"},"source":["plt.plot(malstm_bi_no_comb_trained.history['accuracy'], color ='blue');\n","plt.plot(malstm_bi_no_comb_trained.history['val_accuracy'], color='red');\n","plt.title('Training Accuracy Vs Validation Accuracy');\n","plt.xlabel('Epochs');\n","plt.ylabel('Accuracy');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWu3fEKTl8ij"},"source":["incorrects = malstm_bi_no_comb.predict([q1_test_data, q2_test_data], verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nNJOG0Tfl9TV","executionInfo":{"status":"ok","timestamp":1638006114171,"user_tz":-480,"elapsed":35,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}}},"source":["incorrects[incorrects > 0.5] = 1\n","incorrects[incorrects <= 0.5] = 0\n","flattened_incorrect = incorrects.flatten()"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"-goWfDLpmBgP"},"source":["heatmap_data = confusion_matrix(y_test, flattened_incorrect)\n","group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n","group_counts = [\"{0}\".format(value) for value in heatmap_data.flatten()]\n","group_percentages = [\"{0:.2%}\".format(value) for value in heatmap_data.flatten()/np.sum(heatmap_data)]\n","labels = [f\"{group_name}\\n{group_count}\\n{group_percentage}\" for group_name, group_count, group_percentage in zip(group_names,group_counts,group_percentages)]\n","labels = np.asarray(labels).reshape(2,2)\n","\n","sns.heatmap(heatmap_data, annot=labels, fmt=\"\", cmap='Blues')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v_YpvwGWmDzF"},"source":["tn, fp, fn, tp = confusion_matrix(y_test, flattened_incorrect).ravel()\n","Precision = tp / (tp + fp)\n","Accuracy = (tp + tn) / (tp + tn + fp + fn)\n","Recall = tp / (tp + fn)\n","F1 = 2 / ((1/Recall) + (1/Precision))\n","\n","print(f\"Precision: {Precision} \\nAccuracy: {Accuracy} \\nRecall: {Recall} \\nF1: {F1}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w_hfvQVtrS-b"},"source":["### Penn Han\n","Siamese RNN with LSTM, Bidirectional with Dense Layers"]},{"cell_type":"code","metadata":{"id":"Gzg6afa7rYHq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638006114461,"user_tz":-480,"elapsed":51,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"c3dc5c28-7135-4da7-e367-9f1b03c1c9d6"},"source":["def exponent_neg_manhattan_distance(left, right):\n","    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n","\n","\n","q1_input = Input(shape=(MAX_SEQUENCE,))  #25-dim vectors \n","q2_input = Input(shape=(MAX_SEQUENCE,))  #25-dim vectors\n","\n","LSTM_DIM = 128\n","LSTM_DROPOUT = 0.10\n","LSTM_REGULARIZATION = 0.00\n","\n","q1_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q1_input)  #Creates an Embedding Object and feeds question1 into it\n","q2_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q2_input)\n","\n","manhatten_LSTM = Bidirectional(LSTM(LSTM_DIM, kernel_regularizer=l2(LSTM_REGULARIZATION), dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT))\n","q1_LSTM_Output = manhatten_LSTM(q1_LSTM)\n","q2_LSTM_Output = manhatten_LSTM(q2_LSTM)\n","\n","concatenated = Concatenate()([q1_LSTM_Output, q2_LSTM_Output])\n","concatenated = Dropout(DROPOUT)(concatenated)\n","concatenated = BatchNormalization()(concatenated)\n","\n","concatenated = Dense(150, kernel_regularizer=l2(REGULARIZER), activation='relu')(concatenated)\n","concatenated = Dropout(DROPOUT)(concatenated)\n","concatenated = BatchNormalization()(concatenated)\n","\n","concatenated = Dense(70, kernel_regularizer=l2(REGULARIZER), activation='relu')(concatenated)\n","concatenated = Dropout(DROPOUT)(concatenated)\n","concatenated = BatchNormalization()(concatenated)\n","\n","concatenated = Dense(35, kernel_regularizer=l2(REGULARIZER), activation='relu')(concatenated)\n","concatenated = Dropout(DROPOUT)(concatenated)\n","concatenated = BatchNormalization()(concatenated)\n","\n","output = Dense(1, activation='sigmoid')(concatenated)\n","\n","malstm_bi_comb = Model([q1_input, q2_input], [output])\n","malstm_bi_comb.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n","malstm_bi_comb.summary()"],"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_7 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","input_8 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","embedding_6 (Embedding)         (None, 25, 300)      6000300     input_7[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_7 (Embedding)         (None, 25, 300)      6000300     input_8[0][0]                    \n","__________________________________________________________________________________________________\n","bidirectional_1 (Bidirectional) (None, 256)          439296      embedding_6[0][0]                \n","                                                                 embedding_7[0][0]                \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][0]            \n","                                                                 bidirectional_1[1][0]            \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 512)          0           concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_4 (BatchNor (None, 512)          2048        dropout_4[0][0]                  \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 150)          76950       batch_normalization_4[0][0]      \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 150)          0           dense_4[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_5 (BatchNor (None, 150)          600         dropout_5[0][0]                  \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 70)           10570       batch_normalization_5[0][0]      \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 70)           0           dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_6 (BatchNor (None, 70)           280         dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 35)           2485        batch_normalization_6[0][0]      \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 35)           0           dense_6[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_7 (BatchNor (None, 35)           140         dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","dense_7 (Dense)                 (None, 1)            36          batch_normalization_7[0][0]      \n","==================================================================================================\n","Total params: 12,533,005\n","Trainable params: 530,871\n","Non-trainable params: 12,002,134\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"nb7Eo8vZrYHq"},"source":["print(\"Starting training at\", datetime.datetime.now())\n","t0 = time.time()\n","\n","callbacks_BI_COMB_MLSTM = [ModelCheckpoint(MODEL_WEIGHTS_BIDIRECTIONAL_HONEYCOMB, monitor='val_accuracy', save_best_only=True)]\n","malstm_bi_comb_trained = malstm_bi_comb.fit([q1_train_data, q2_train_data],\n","                            y_train,\n","                            batch_size=BATCH_SIZE,\n","                            epochs=NUM_EPOCHS,\n","                            validation_split=VALIDATION,\n","                            verbose = 1,\n","                            callbacks = callbacks_BI_COMB_MLSTM)\n","\n","t1 = time.time()\n","print(\"Training ended at\", datetime.datetime.now())\n","print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kwT6KhpgrYHq"},"source":["malstm_bi_comb.save(\"MALSTM_BI_COMB\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c68poCwNrYHq"},"source":["max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(malstm_bi_comb_trained.history['val_accuracy']))\n","print('Maximum validation accuracy = {0:.4f} (epoch {1:d})'.format(max_val_acc, idx+1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FdxxcITWrYHq"},"source":["malstm_bi_comb.load_weights(MODEL_WEIGHTS_BIDIRECTIONAL_HONEYCOMB)\n","loss, accuracy = malstm_bi_comb.evaluate([q1_test_data, q2_test_data], y_test, verbose=1)\n","print('Test loss = {0:.4f}, test accuracy = {1:.4f}'.format(loss, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7zriQtxOrYHq"},"source":["plt.plot(malstm_bi_comb_trained.history['loss'], color ='blue');\n","plt.plot(malstm_bi_comb_trained.history['val_loss'], color='red');\n","plt.title('Training Loss Vs Validation Loss');\n","plt.xlabel('Epochs');\n","plt.ylabel('Loss');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"geKraT2irYHr"},"source":["plt.plot(malstm_bi_comb_trained.history['accuracy'], color ='blue');\n","plt.plot(malstm_bi_comb_trained.history['val_accuracy'], color='red');\n","plt.title('Training Accuracy Vs Validation Accuracy');\n","plt.xlabel('Epochs');\n","plt.ylabel('Accuracy');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dU56WkjBrYHr"},"source":["incorrects = malstm_bi_comb.predict([q1_test_data, q2_test_data], verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFfCn5GsrYHr","executionInfo":{"status":"ok","timestamp":1638008043854,"user_tz":-480,"elapsed":63,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}}},"source":["incorrects[incorrects > 0.5] = 1\n","incorrects[incorrects <= 0.5] = 0\n","flattened_incorrect = incorrects.flatten()"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"hNUQR8jwrYHr"},"source":["heatmap_data = confusion_matrix(y_test, flattened_incorrect)\n","group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n","group_counts = [\"{0}\".format(value) for value in heatmap_data.flatten()]\n","group_percentages = [\"{0:.2%}\".format(value) for value in heatmap_data.flatten()/np.sum(heatmap_data)]\n","labels = [f\"{group_name}\\n{group_count}\\n{group_percentage}\" for group_name, group_count, group_percentage in zip(group_names,group_counts,group_percentages)]\n","labels = np.asarray(labels).reshape(2,2)\n","\n","sns.heatmap(heatmap_data, annot=labels, fmt=\"\", cmap='Blues')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tz4MBn5YrYHr"},"source":["tn, fp, fn, tp = confusion_matrix(y_test, flattened_incorrect).ravel()\n","Precision = tp / (tp + fp)\n","Accuracy = (tp + tn) / (tp + tn + fp + fn)\n","Recall = tp / (tp + fn)\n","F1 = 2 / ((1/Recall) + (1/Precision))\n","\n","print(f\"Precision: {Precision} \\nAccuracy: {Accuracy} \\nRecall: {Recall} \\nF1: {F1}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gO60gyuq6DNv"},"source":["### Yong Shen\n","Siamese RNN with LSTM, with Features"]},{"cell_type":"code","metadata":{"id":"V4a_CeP46H7O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638008044891,"user_tz":-480,"elapsed":364,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}},"outputId":"4d05e5a0-6800-4bcc-a91b-58b7fbada20f"},"source":["def exponent_neg_manhattan_distance(left, right):\n","    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n","\n","\n","q1_input = Input(shape=(MAX_SEQUENCE,))  #25-dim vectors \n","q2_input = Input(shape=(MAX_SEQUENCE,))  #25-dim vectors\n","\n","LSTM_DIM = 128\n","LSTM_DROPOUT = 0.10\n","LSTM_REGULARIZATION = 0.0001\n","\n","q1_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q1_input)  #Creates an Embedding Object and feeds question1 into it\n","q2_LSTM = Embedding(number_of_words + 1,\n","               EMBED_DIM,\n","               weights=[word_embedding_matrix],\n","               input_length=MAX_SEQUENCE,\n","               trainable=False)(q2_input)\n","\n","\n","manhatten_LSTM = LSTM(LSTM_DIM, kernel_regularizer=l2(LSTM_REGULARIZATION), dropout=LSTM_DROPOUT, recurrent_dropout=LSTM_DROPOUT)\n","q1_LSTM_Output = manhatten_LSTM(q1_LSTM)\n","q2_LSTM_Output = manhatten_LSTM(q2_LSTM)\n","\n","malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([q1_LSTM_Output, q2_LSTM_Output])  #What is x[0][0]??\n","\n","no_of_features = len(X_train_features.columns)\n","feature_input = Input(shape=(no_of_features,))\n","feature_layer = Dense(70, activation='relu')(feature_input)\n","\n","concat_layer = Concatenate()([q1_LSTM_Output, q2_LSTM_Output, feature_layer])\n","concat_layer = Dropout(DROPOUT)(concat_layer)\n","concat_layer = BatchNormalization()(concat_layer)\n","\n","concat_layer = Dense(150, kernel_regularizer=l2(REGULARIZER), activation='relu')(concat_layer)\n","concat_layer = Dropout(DROPOUT)(concat_layer)\n","concat_layer = BatchNormalization()(concat_layer)\n","\n","concat_layer = Dense(70, kernel_regularizer=l2(REGULARIZER), activation='relu')(concat_layer)\n","concat_layer = Dropout(DROPOUT)(concat_layer)\n","concat_layer = BatchNormalization()(concat_layer)\n","\n","concat_layer = Dense(35, kernel_regularizer=l2(REGULARIZER), activation='relu')(concat_layer)\n","concat_layer = Dropout(DROPOUT)(concat_layer)\n","concat_layer = BatchNormalization()(concat_layer)\n","\n","output = Dense(1, activation='sigmoid')(concat_layer)\n","\n","malstm_w_feature = Model([q1_input, q2_input, feature_input], [output])\n","malstm_w_feature.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n","malstm_w_feature.summary()"],"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_4\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_9 (InputLayer)            [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","input_10 (InputLayer)           [(None, 25)]         0                                            \n","__________________________________________________________________________________________________\n","embedding_8 (Embedding)         (None, 25, 300)      6000300     input_9[0][0]                    \n","__________________________________________________________________________________________________\n","embedding_9 (Embedding)         (None, 25, 300)      6000300     input_10[0][0]                   \n","__________________________________________________________________________________________________\n","input_11 (InputLayer)           [(None, 22)]         0                                            \n","__________________________________________________________________________________________________\n","lstm_4 (LSTM)                   (None, 128)          219648      embedding_8[0][0]                \n","                                                                 embedding_9[0][0]                \n","__________________________________________________________________________________________________\n","dense_8 (Dense)                 (None, 70)           1610        input_11[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 326)          0           lstm_4[0][0]                     \n","                                                                 lstm_4[1][0]                     \n","                                                                 dense_8[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_8 (Dropout)             (None, 326)          0           concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_8 (BatchNor (None, 326)          1304        dropout_8[0][0]                  \n","__________________________________________________________________________________________________\n","dense_9 (Dense)                 (None, 150)          49050       batch_normalization_8[0][0]      \n","__________________________________________________________________________________________________\n","dropout_9 (Dropout)             (None, 150)          0           dense_9[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization_9 (BatchNor (None, 150)          600         dropout_9[0][0]                  \n","__________________________________________________________________________________________________\n","dense_10 (Dense)                (None, 70)           10570       batch_normalization_9[0][0]      \n","__________________________________________________________________________________________________\n","dropout_10 (Dropout)            (None, 70)           0           dense_10[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_10 (BatchNo (None, 70)           280         dropout_10[0][0]                 \n","__________________________________________________________________________________________________\n","dense_11 (Dense)                (None, 35)           2485        batch_normalization_10[0][0]     \n","__________________________________________________________________________________________________\n","dropout_11 (Dropout)            (None, 35)           0           dense_11[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_11 (BatchNo (None, 35)           140         dropout_11[0][0]                 \n","__________________________________________________________________________________________________\n","dense_12 (Dense)                (None, 1)            36          batch_normalization_11[0][0]     \n","==================================================================================================\n","Total params: 12,286,323\n","Trainable params: 284,561\n","Non-trainable params: 12,001,762\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"nfbEYhGf6NTx"},"source":["print(\"Starting training at\", datetime.datetime.now())\n","t0 = time.time()\n","\n","callbacks_MLSTM_feature = [ModelCheckpoint(MODEL_WEIGHTS_FILE_MLSTM_FEATURE, monitor='val_accuracy', save_best_only=True)]\n","malstm_w_feature_trained = malstm_w_feature.fit([q1_train_data, q2_train_data, X_train_features],\n","                            y_train,\n","                            batch_size=BATCH_SIZE,\n","                            epochs=NUM_EPOCHS,\n","                            validation_split=VALIDATION,\n","                            verbose = 1,\n","                            callbacks = callbacks_MLSTM_feature)\n","\n","t1 = time.time()\n","print(\"Training ended at\", datetime.datetime.now())\n","print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Pkwz94NWoUJ"},"source":["malstm_w_feature.save(\"MALTSM_W_FEATURES\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QwYXoGGC6g5s"},"source":["max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(malstm_w_feature_trained.history['val_accuracy']))\n","print('Maximum validation accuracy = {0:.4f} (epoch {1:d})'.format(max_val_acc, idx+1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WnXjpQ0J6jOy"},"source":["malstm_w_feature.load_weights(MODEL_WEIGHTS_FILE_MLSTM_FEATURE)\n","loss, accuracy = malstm_w_feature.evaluate([q1_test_data, q2_test_data, X_test_features], y_test, verbose=1)\n","print('Test loss = {0:.4f}, test accuracy = {1:.4f}'.format(loss, accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3yhsyM7VtlpS"},"source":["plt.plot(malstm_w_feature_trained.history['loss'], color ='blue');\n","plt.plot(malstm_w_feature_trained.history['val_loss'], color='red');\n","plt.title('Training Loss Vs Validation Loss');\n","plt.xlabel('Epochs');\n","plt.ylabel('Loss');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wt1fjHbttlpS"},"source":["plt.plot(malstm_w_feature_trained.history['accuracy'], color ='blue');\n","plt.plot(malstm_w_feature_trained.history['val_accuracy'], color='red');\n","plt.title('Training Accuracy Vs Validation Accuracy');\n","plt.xlabel('Epochs');\n","plt.ylabel('Accuracy');"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GaSNFOM_tlpS"},"source":["incorrects = malstm_w_feature.predict([q1_test_data, q2_test_data, X_test_features], verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x4ujlq4PtlpS","executionInfo":{"status":"ok","timestamp":1638011034622,"user_tz":-480,"elapsed":27,"user":{"displayName":"Penn Han Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06071372039627482089"}}},"source":["incorrects[incorrects > 0.5] = 1\n","incorrects[incorrects <= 0.5] = 0\n","flattened_incorrect = incorrects.flatten()"],"execution_count":79,"outputs":[]},{"cell_type":"code","metadata":{"id":"nZbHd_L0uFYQ"},"source":["heatmap_data = confusion_matrix(y_test, flattened_incorrect)\n","group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n","group_counts = [\"{0}\".format(value) for value in heatmap_data.flatten()]\n","group_percentages = [\"{0:.2%}\".format(value) for value in heatmap_data.flatten()/np.sum(heatmap_data)]\n","labels = [f\"{group_name}\\n{group_count}\\n{group_percentage}\" for group_name, group_count, group_percentage in zip(group_names,group_counts,group_percentages)]\n","labels = np.asarray(labels).reshape(2,2)\n","\n","sns.heatmap(heatmap_data, annot=labels, fmt=\"\", cmap='Blues')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S2rROWb1tlpT"},"source":["tn, fp, fn, tp = confusion_matrix(y_test, flattened_incorrect).ravel()\n","Precision = tp / (tp + fp)\n","Accuracy = (tp + tn) / (tp + tn + fp + fn)\n","Recall = tp / (tp + fn)\n","F1 = 2 / ((1/Recall) + (1/Precision))\n","\n","print(f\"Precision: {Precision} \\nAccuracy: {Accuracy} \\nRecall: {Recall} \\nF1: {F1}\")"],"execution_count":null,"outputs":[]}]}