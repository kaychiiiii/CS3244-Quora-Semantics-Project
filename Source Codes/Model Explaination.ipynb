{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Model Explaination.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1opclJNtzOZS"},"source":["# Colab for Feature Engineering.  #\n","\n","---\n","\n","\n","## To Do: ##\n","1. Separate the Duplicates and Non-Duplicates\n","2. Copy the DataFrames into new variables\n","3. Process the data into the feature to be extracted\n","4. Plot the Histogram to see to observe the distribution\n","\n","\n","---\n","\n","\n","##Features##\n","\n","1. Number of unique words which occur in q1 and q2 \n","2. Ratio of common words / total words (q1+q2)\n","2. Common Word Ratio min ( words common/ min(len(q1), len(q2)))\n","2. Common Word Ratio mmax ( words common/ max(len(q1), len(q2)))\n","2. Common Stop Words min ( common stopwords/ min(len(q1), len(q2)))\n","2. Common Stop Words max  ( common stopwords/ max(len(q1), len(q2)))\n","2. Common Tokens min ( common Tokens / min(len(q1), len(q2)))\n","2. Common Tokens max  ( common Tokens / max(len(q1), len(q2)))\n","2. Common Adjectives min ( common adjectives /min(len(q1), len(q2)))\n","2. Common Adjectives max ( common adjectives /max(len(q1), len(q2)))\n","2. Common Noun min ( common nouns / min(len(q1), len(q2)))\n","2. Common Noun max ( common nouns / max(len(q1), len(q2)))\n","2. Fuzz ratio\n","2. Fuzz partial ratio \n","2. Fuzz Token Sort Ratio \n","2. Fuzz Token Set Ratio\n","2. Mean Length of 2 questions\n","2. Ratio of Length of Questions ( len(q1) / len(q2) )\n","2. Absolute Length Difference (| len(q1) - len(q2) |\n","2. Longest Matching Substring min ( longest substring/min(len(q1), len(q2)))\n","2. Longest Matching Substring max ( longest substring/max(len(q1), len(q2)))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aitd3DXdGW02"},"source":["Download your required libraries here"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cp6xm-95GQR6","executionInfo":{"status":"ok","timestamp":1637999296346,"user_tz":-480,"elapsed":193562,"user":{"displayName":"Neaton Ang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04864092167858940876"}},"outputId":"f62a8faf-6be8-4e98-8d89-98446569aea1"},"source":["!pip install bs4\n","!pip install fuzzywuzzy\n","!pip install TextBlob\n","!pip install pickle5\n","!python -m spacy download en_core_web_lg\n","!pip install keras==2.6.0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n","Collecting fuzzywuzzy\n","  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n","Installing collected packages: fuzzywuzzy\n","Successfully installed fuzzywuzzy-0.18.0\n","Requirement already satisfied: TextBlob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from TextBlob) (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->TextBlob) (1.15.0)\n","Collecting pickle5\n","  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n","\u001b[K     |████████████████████████████████| 256 kB 12.6 MB/s \n","\u001b[?25hInstalling collected packages: pickle5\n","Successfully installed pickle5-0.0.12\n","Collecting en_core_web_lg==2.2.5\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9 MB)\n","\u001b[K     |████████████████████████████████| 827.9 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.62.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (57.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.6)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.6)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.10.0.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n","Building wheels for collected packages: en-core-web-lg\n","  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-py3-none-any.whl size=829180942 sha256=9d50f1c826803fde486e554edc26b13f7b47017066850fc05b27e8336cc8f61c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-adgdwep5/wheels/11/95/ba/2c36cc368c0bd339b44a791c2c1881a1fb714b78c29a4cb8f5\n","Successfully built en-core-web-lg\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_lg')\n","Collecting keras==2.6.0\n","  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 11.9 MB/s \n","\u001b[?25hInstalling collected packages: keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.7.0\n","    Uninstalling keras-2.7.0:\n","      Successfully uninstalled keras-2.7.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.7.0 requires keras<2.8,>=2.7.0rc0, but you have keras 2.6.0 which is incompatible.\u001b[0m\n","Successfully installed keras-2.6.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"urv0kxsD0z-P"},"source":["Import your required libraries here"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2GtU4YrQ0zuM","executionInfo":{"status":"ok","timestamp":1637999445852,"user_tz":-480,"elapsed":14146,"user":{"displayName":"Neaton Ang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04864092167858940876"}},"outputId":"14bccff1-20ef-410a-c661-6a58c7b00de5"},"source":["import pandas as pd\n","import numpy as np\n","import re\n","from bs4 import BeautifulSoup\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from keras.preprocessing.text import Tokenizer\n","import nltk\n","from fuzzywuzzy import fuzz\n","from difflib import SequenceMatcher #For finding longest substring\n","from textblob import TextBlob\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import spacy\n","import en_core_web_lg\n","import pickle5\n","from keras.models import load_model\n","from keras import backend as K\n","from keras.preprocessing.sequence import pad_sequences\n","nlp = spacy.load('en_core_web_lg')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger') # for pos tagging\n","from tqdm import tqdm_notebook\n","from scipy.spatial.distance import cosine\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_score, confusion_matrix"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n","  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"]},{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]}]},{"cell_type":"markdown","metadata":{"id":"0-b9QIRu0bLP"},"source":["Mounting the dataset onto this google colab"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ub9Gpx8Y_e8c","executionInfo":{"status":"ok","timestamp":1637999480873,"user_tz":-480,"elapsed":24196,"user":{"displayName":"Neaton Ang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04864092167858940876"}},"outputId":"753723fa-839c-48a2-d646-b592eeb24f40"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"/content/drive/MyDrive/CS3244 45 Project/RNN Models\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1ixE_YVbTLblbUJgpPcDWfO-zlQeoNGq4/CS3244 45 Project/RNN Models\n"]}]},{"cell_type":"code","metadata":{"id":"DWi8Re2HzG5o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637999546804,"user_tz":-480,"elapsed":65937,"user":{"displayName":"Neaton Ang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04864092167858940876"}},"outputId":"e334aead-0668-4d89-fecb-c109b75eb5d6"},"source":["#Loading the tokenizer\n","with open('../tokenizer.pickle', 'rb') as saved_tokenizer:\n","    tokenizer = pickle5.load(saved_tokenizer)\n","\n","def exponent_neg_manhattan_distance(left, right):\n","    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n","\n","model = load_model('ys.h5')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]}]},{"cell_type":"code","metadata":{"id":"DmNz7ekd_Plk"},"source":["model_initial = load_model('baseline_3')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tgik8RWhM61f"},"source":["##**Preprocess the questions**"]},{"cell_type":"code","metadata":{"id":"01o0b7ktMaUt"},"source":["# This function accepts a question and preprocesses it. Returns cleaned question.\n","# This section of code was referenced from Sourab Vadlamani in his work \"Quora Question Pairs Similairty, Tackling a real life NLP problem\"\n","# https://towardsdatascience.com/quora-question-pairs-similarity-tackling-a-real-life-nlp-problem-ab55c5da2e84\n","\n","def preprocess(q):\n","  # Firstly, we convert to lowercase and remove trailing and leading spaces\n","  q = str(q).lower().strip()\n","\n","  # Replace certain special characters with their string equivalents\n","  q = q.replace('%', ' percent')\n","  q = q.replace('$', ' dollar ')\n","  q = q.replace('₹', ' rupee ')\n","  q = q.replace('€', ' euro ')\n","  q = q.replace('@', ' at ')\n","\n","  # The pattern '[math]' appears around 900 times in the whole dataset.\n","  q = q.replace('[math]', '')\n","\n","  # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n","  q = q.replace(',000,000,000 ', 'b ')\n","  q = q.replace(',000,000 ', 'm ')\n","  q = q.replace(',000 ', 'k ')\n","  q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n","  q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n","  q = re.sub(r'([0-9]+)000', r'\\1k', q)\n","\n","  # Decontracting words\n","  # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n","  # https://stackoverflow.com/a/19794953\n","  contractions = { \n","    \"ain't\": \"am not\",\n","    \"aren't\": \"are not\",\n","    \"can't\": \"can not\",\n","    \"can't've\": \"can not have\",\n","    \"'cause\": \"because\",\n","    \"could've\": \"could have\",\n","    \"couldn't\": \"could not\",\n","    \"couldn't've\": \"could not have\",\n","    \"didn't\": \"did not\",\n","    \"doesn't\": \"does not\",\n","    \"don't\": \"do not\",\n","    \"hadn't\": \"had not\",\n","    \"hadn't've\": \"had not have\",\n","    \"hasn't\": \"has not\",\n","    \"haven't\": \"have not\",\n","    \"he'd\": \"he would\",\n","    \"he'd've\": \"he would have\",\n","    \"he'll\": \"he will\",\n","    \"he'll've\": \"he will have\",\n","    \"he's\": \"he is\",\n","    \"how'd\": \"how did\",\n","    \"how'd'y\": \"how do you\",\n","    \"how'll\": \"how will\",\n","    \"how's\": \"how is\",\n","    \"i'd\": \"i would\",\n","    \"i'd've\": \"i would have\",\n","    \"i'll\": \"i will\",\n","    \"i'll've\": \"i will have\",\n","    \"i'm\": \"i am\",\n","    \"i've\": \"i have\",\n","    \"isn't\": \"is not\",\n","    \"it'd\": \"it would\",\n","    \"it'd've\": \"it would have\",\n","    \"it'll\": \"it will\",\n","    \"it'll've\": \"it will have\",\n","    \"it's\": \"it is\",\n","    \"let's\": \"let us\",\n","    \"ma'am\": \"madam\",\n","    \"mayn't\": \"may not\",\n","    \"might've\": \"might have\",\n","    \"mightn't\": \"might not\",\n","    \"mightn't've\": \"might not have\",\n","    \"must've\": \"must have\",\n","    \"mustn't\": \"must not\",\n","    \"mustn't've\": \"must not have\",\n","    \"needn't\": \"need not\",\n","    \"needn't've\": \"need not have\",\n","    \"o'clock\": \"of the clock\",\n","    \"oughtn't\": \"ought not\",\n","    \"oughtn't've\": \"ought not have\",\n","    \"shan't\": \"shall not\",\n","    \"sha'n't\": \"shall not\",\n","    \"shan't've\": \"shall not have\",\n","    \"she'd\": \"she would\",\n","    \"she'd've\": \"she would have\",\n","    \"she'll\": \"she will\",\n","    \"she'll've\": \"she will have\",\n","    \"she's\": \"she is\",\n","    \"should've\": \"should have\",\n","    \"shouldn't\": \"should not\",\n","    \"shouldn't've\": \"should not have\",\n","    \"so've\": \"so have\",\n","    \"so's\": \"so as\",\n","    \"that'd\": \"that would\",\n","    \"that'd've\": \"that would have\",\n","    \"that's\": \"that is\",\n","    \"there'd\": \"there would\",\n","    \"there'd've\": \"there would have\",\n","    \"there's\": \"there is\",\n","    \"they'd\": \"they would\",\n","    \"they'd've\": \"they would have\",\n","    \"they'll\": \"they will\",\n","    \"they'll've\": \"they will have\",\n","    \"they're\": \"they are\",\n","    \"they've\": \"they have\",\n","    \"to've\": \"to have\",\n","    \"wasn't\": \"was not\",\n","    \"we'd\": \"we would\",\n","    \"we'd've\": \"we would have\",\n","    \"we'll\": \"we will\",\n","    \"we'll've\": \"we will have\",\n","    \"we're\": \"we are\",\n","    \"we've\": \"we have\",\n","    \"weren't\": \"were not\",\n","    \"what'll\": \"what will\",\n","    \"what'll've\": \"what will have\",\n","    \"what're\": \"what are\",\n","    \"what's\": \"what is\",\n","    \"what've\": \"what have\",\n","    \"when's\": \"when is\",\n","    \"when've\": \"when have\",\n","    \"where'd\": \"where did\",\n","    \"where's\": \"where is\",\n","    \"where've\": \"where have\",\n","    \"who'll\": \"who will\",\n","    \"who'll've\": \"who will have\",\n","    \"who's\": \"who is\",\n","    \"who've\": \"who have\",\n","    \"why's\": \"why is\",\n","    \"why've\": \"why have\",\n","    \"will've\": \"will have\",\n","    \"won't\": \"will not\",\n","    \"won't've\": \"will not have\",\n","    \"would've\": \"would have\",\n","    \"wouldn't\": \"would not\",\n","    \"wouldn't've\": \"would not have\",\n","    \"y'all\": \"you all\",\n","    \"y'all'd\": \"you all would\",\n","    \"y'all'd've\": \"you all would have\",\n","    \"y'all're\": \"you all are\",\n","    \"y'all've\": \"you all have\",\n","    \"you'd\": \"you would\",\n","    \"you'd've\": \"you would have\",\n","    \"you'll\": \"you will\",\n","    \"you'll've\": \"you will have\",\n","    \"you're\": \"you are\",\n","    \"you've\": \"you have\"\n","  }\n","\n","  q_decontracted = []\n","\n","  for word in q.split():\n","    if word in contractions:\n","      word = contractions[word]\n","  \n","    q_decontracted.append(word)\n","\n","  q = ' '.join(q_decontracted)\n","  q = q.replace(\"'ve\", \" have\")\n","  q = q.replace(\"n't\", \" not\")\n","  q = q.replace(\"'re\", \" are\")\n","  q = q.replace(\"'ll\", \" will\")\n","\n","  # Removing HTML tags\n","  q = BeautifulSoup(q)\n","  q = q.get_text()\n","\n","  # Remove punctuations\n","  pattern = re.compile('\\W')\n","  q = re.sub(pattern, ' ', q).strip()\n","\n","  return q"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"chHSSf-lOAK9"},"source":["##**Jun An**\n","\n","1. Ratio of Common Words (Common words / total words) (Done)\n","2. Ratio of Common Tokens (Common tokens/ max(q1, q2)) (Done)\n","3. Fuzz partial ratio (Done)\n","4. Longest Matching Substring Min (Done)"]},{"cell_type":"code","metadata":{"id":"4kNBhBLUNUjz"},"source":["def num_common_words_ratio(row):\n","  set1 = set(row['question1'].lower().split())\n","  set2 = set(row['question2'].lower().split())\n","  total = len(set1) + len(set2)\n","  return len(set1.intersection(set2))/total"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"38QSMNbEQVhh"},"source":["def common_tokens_ratio_max(row):\n","  q1 = set(word_tokenize(row['question1'].lower()))\n","  q2 = set(word_tokenize(row['question2'].lower()))\n","  stop_words = set(stopwords.words('english'))\n","  token1 = [word for word in q1 if word not in stop_words]\n","  token2 = [word for word in q2 if word not in stop_words]\n","  ratio = len(set(token1).intersection(set(token2))) / max(len(row['question1']), len(row['question2']))\n","\n","  return ratio\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZuilUh04Urcu"},"source":["def fuzz_partial_ratio(row):\n","  q1 = row['question1']\n","  q2 = row['question2']\n","  fuzz_partial = fuzz.partial_ratio(q1,q2)\n","  return fuzz_partial"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B7Z_z7fHelbz"},"source":["def min_longest_substring(row):\n","  q1 = row['question1']\n","  q2 = row['question2']\n","  match = SequenceMatcher(None, q1, q2).find_longest_match(0, len(q1), 0, len(q2))\n","  return match.size/min(len(q1), len(q2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7E95ziqm9V2X"},"source":["##**Penn Han**\n","\n","1. Number of unique words that occur in q1 and q2\n","2. Ratio of Common Tokens to min(len(q1), len(q2))\n","3. Fuzz Ratio\n","4. Absolute Length Difference between q1 and q2\n","5. Mean TF-IDF value\n","6. Mean IDF-weighted vector"]},{"cell_type":"code","metadata":{"id":"l2MxJruy92k6"},"source":["def unique_words_count(row):\n","  set1 = set(row['question1'].lower().split())\n","  set2 = set(row['question2'].lower().split())\n","  return len(set1.intersection(set2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7CGxW3M99dO"},"source":["def common_token_ratio_min(row):\n","  q1 = set(word_tokenize(row['question1'].lower()))\n","  q2 = set(word_tokenize(row['question2'].lower()))\n","  stop_words = set(stopwords.words('english'))\n","  token1 = [word for word in q1 if word not in stop_words]\n","  token2 = [word for word in q2 if word not in stop_words]\n","  ratio = len(set(token1).intersection(set(token2))) / min(len(row['question1']), len(row['question2']))\n","  return ratio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3tve1rwe-J91"},"source":["def fuzz_ratio(row):\n","  q1 = row['question1']\n","  q2 = row['question2']\n","  fuzz_ratio = fuzz.ratio(q1,q2)\n","  return fuzz_ratio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ns-DI5-q-NDP"},"source":["def abs_len_difference(row):\n","  q1 = row['question1']\n","  q2 = row['question2']\n","  abs_len_diff = abs(len(q1) - len(q2))\n","  return abs_len_diff"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H7_C-NlBvGHx"},"source":["#Stop words not removed PLEASE ONLY USE EITHER THIS OR THE BELOW, NOT BOTH\n","\n","#tf_idf_vectoriser = TfidfVectorizer(lowercase=True)\n","#q1_train_list = list(train_set['question1'])\n","#q2_train_list = list(train_set['question2'])\n","#question_corpus = list(q1_train_list + q2_train_list)\n","#tf_idf_vectoriser.fit(question_corpus)\n","#idf = dict(zip(tf_idf_vectoriser.get_feature_names(), tf_idf_vectoriser.idf_))  #For Weighted W2V\n","#nlp = en_core_web_lg.load()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UY0LWFd4gp9w"},"source":["def mean_tfidf_value_q1(row):\n","  q1 = word_tokenize(row['question1'].lower())\n","  stop_words = set(stopwords.words('english'))\n","  token1 = [word for word in q1 if word not in stop_words]\n","  if len(token1) > 0:\n","    q1_vector_matrix = tf_idf_vectoriser.transform(token1)  #Transform must take in a iterable so [str]\n","    return q1_vector_matrix  #Returns a sparse matrix\n","  else:\n","    return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TvAvWQQFlHzT"},"source":["def mean_tfidf_value_q2(row):\n","  q2 = set(word_tokenize(row['question2'].lower()))\n","  stop_words = set(stopwords.words('english'))\n","  token2 = [word for word in q1 if word not in stop_words]\n","  if len(token1) > 0:\n","    q2_vector_matrix = tf_idf_vectoriser.transform(token2)  #Transform must take in a iterable so [str]\n","    return q2_vector_matrix  #Returns a sparse matrix\n","  else:\n","    return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Tpg01HjwWHb"},"source":["def calculate_weighted_vector(question):\n","    weighted_vectors = []\n","    doc = nlp(question)\n","    mean_vec = np.zeros((len(doc[0].vector)))\n","    for word in doc:\n","        vector = word.vector\n","        if str(word) in idf:\n","            idf_weight = idf[str(word)]\n","        else:\n","            idf_weight = 0\n","        mean_vec += vector * idf_weight\n","    mean_vec /= len(doc)\n","    return mean_vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E-q_ZSyigxlu"},"source":["def mean_idfweighted_vector_q1(row):\n","  idfweighted_vector_q1 = calculate_weighted_vector(row['question1'])\n","  return idfweighted_vector_q1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b3blVuU5lJi1"},"source":["def mean_idfweighted_vector_q2(row):\n","  idfweighted_vector_q2 = calculate_weighted_vector(row['question2'])\n","  return idfweighted_vector_q2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QdrF_cOXguKF"},"source":["#train_set[\"tfidf_matrix_q1\"] = train_set.apply(mean_tfidf_value_q1, axis=1)\n","#train_set[\"tfidf_matrix_q2\"] = train_set.apply(mean_tfidf_value_q2, axis=1)\n","#train_set[\"mean_idfweighted_vector_q1\"] = train_set.apply(mean_idfweighted_vector_q1, axis=1)\n","#train_set[\"mean_idfweighted_vector_q2\"] = train_set.apply(mean_idfweighted_vector_q2, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YCM7dbHZpXVA"},"source":["## Jeremy\n","1. common stop words min\n","2. common noun min\n","3. mean length of 2 questions"]},{"cell_type":"code","metadata":{"id":"kP_v45Qo5sz7"},"source":["stop_words = set(stopwords.words('english'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O99NJaXp6Ic6"},"source":["def get_min_len_qn(row):\n","    return min(len(row['question1'].split()), len(row['question2'].split()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a3fp8Mol6Jcu"},"source":["def calc_common_stop_words_min(row):\n","    q1 = word_tokenize(row['question1'])\n","    q2 = word_tokenize(row['question2'])\n","    stop_words_q1 = set([x for x in q1 if x in stop_words])\n","    stop_words_q2 = set([x for x in q2 if x in stop_words])\n","    num_intersect = len(stop_words_q1.intersection(stop_words_q2))\n","    return num_intersect / get_min_len_qn(row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_SFMAd86L3J"},"source":["def calc_common_nouns_min(row):\n","    q1_tokens = word_tokenize(row[\"question1\"].lower())\n","    q2_tokens = word_tokenize(row[\"question2\"].lower())\n","    pos_tagged_q1 = nltk.pos_tag(q1_tokens)\n","    pos_tagged_q2 = nltk.pos_tag(q2_tokens)\n","    # x[0] is the word, x[1] is the tag\n","    q1_nouns = set([x[0] for x in pos_tagged_q1 if x[1] == \"NN\"]) \n","    q2_nouns = set([x[0] for x in pos_tagged_q2 if x[1] == \"NN\"])\n","    return len(q1_nouns.intersection(q2_nouns)) / get_min_len_qn(row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I0Lg4pms6QT2"},"source":["def mean_len_qns(row):\n","    return (len(word_tokenize(row[\"question1\"].lower())) + len(word_tokenize(row[\"question2\"].lower()))) / 2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hE50X--UjtRR"},"source":["##Kay Chi\n","1. Common stop words max\n","2. Common noun max\n","3. Ratio of length of questions"]},{"cell_type":"code","metadata":{"id":"r_D34iHTtILS"},"source":["def get_max_len_qn(row):\n","    return max(len(row['question1'].split()), len(row['question2'].split()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bou-DlcWtIC_"},"source":["def calc_common_stop_words_max(row):\n","    q1 = word_tokenize(row['question1'])\n","    q2 = word_tokenize(row['question2'])\n","    stop_words_q1 = set([x for x in q1 if x in stop_words])\n","    stop_words_q2 = set([x for x in q2 if x in stop_words])\n","    num_intersect = len(stop_words_q1.intersection(stop_words_q2))\n","    return num_intersect / get_max_len_qn(row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8skmwJPawOix"},"source":["def calc_common_nouns_max(row):\n","    q1_tokens = word_tokenize(row[\"question1\"].lower())\n","    q2_tokens = word_tokenize(row[\"question2\"].lower())\n","    pos_tagged_q1 = nltk.pos_tag(q1_tokens)\n","    pos_tagged_q2 = nltk.pos_tag(q2_tokens)\n","    # x[0] is the word, x[1] is the tag\n","    q1_nouns = set([x[0] for x in pos_tagged_q1 if x[1] == \"NN\"]) \n","    q2_nouns = set([x[0] for x in pos_tagged_q2 if x[1] == \"NN\"])\n","    return len(q1_nouns.intersection(q2_nouns)) / get_max_len_qn(row)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"58E1CpXM4SXM"},"source":["def ratio_len_qn(row):\n","  q1 = row['question1']\n","  q2 = row['question2']\n","  return len(q1) / len(q2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wy4iRdRk1toC"},"source":["## YS\n","1. Common Word Ratio max ( words common/ max(len(q1), len(q2))) \n","2. Common Adjectives max ( common adjectives /max(len(q1), len(q2)))\n","3. Fuzz Token Set Ratio "]},{"cell_type":"code","metadata":{"id":"chI-Q-MP2Ljt"},"source":["def common_word_ratio_max(row):\n","  q1 = row['question1']\n","  q2 = row['question2']\n","  return len(set(q1).intersection(set(q2))) / max(len(q1), len(q2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6waA2IaX3GkL"},"source":["# This has been tested to be correct, but result seems off.\n","def get_adjectives(text):\n","  blob = TextBlob(text)\n","  return set(word for (word,tag) in blob.tags if tag.startswith(\"JJ\"))\n","  \n","def common_adjectives_max(row):\n","  q1 = row['question1']\n","  q2 = row['question2']\n","  return len(get_adjectives(q1).intersection(get_adjectives(q2))) / max(len(q1), len(q2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2dn4sZUH8Awt"},"source":["def calc_fuzz_token_set_ratio(row):\n","  q1 = row['question1']\n","  q2 = row['question2']\n","  return fuzz.token_set_ratio(q1, q2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hgxQeK8TqH8m"},"source":["##**Neaton**"]},{"cell_type":"code","metadata":{"id":"914YiTxLPiyl"},"source":["def common_words_ratio_min(row):\n","  set1 = set(row['question1'].lower().split())\n","  set2 = set(row['question2'].lower().split())\n","  common_words = len(set1.intersection(set2))\n","  return common_words/min(len(set1), len(set2))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P4kBabFxrbRn"},"source":["# This has been tested to be correct, but result seems off.\n","def get_adjectives(text):\n","  blob = TextBlob(text)\n","  return set(word for (word,tag) in blob.tags if tag.startswith(\"JJ\"))\n","  \n","def common_adjectives_min(row):\n","  q1 = row['question1']\n","  q2 = row['question2']\n","  return len(get_adjectives(q1).intersection(get_adjectives(q2))) / min(len(q1), len(q2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3JJWQIwVr3Kr"},"source":["def fuzz_token_sort_ratio(row):\n","  q1 = row['question1']\n","  q2 = row['question2']\n","  fuzz_token = fuzz.token_sort_ratio(q1,q2)\n","  return fuzz_token"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Qb_zvHSsL0r"},"source":["def max_longest_substring(row):\n","  q1 = row['question1']\n","  q2 = row['question2']\n","  match = SequenceMatcher(None, q1, q2).find_longest_match(0, len(q1), 0, len(q2))\n","  return match.size/max(len(q1), len(q2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k6ASmaXB3DUV"},"source":["##Loading GloVe Embedding"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o6eu8L058FGd","executionInfo":{"status":"ok","timestamp":1637999556619,"user_tz":-480,"elapsed":15,"user":{"displayName":"Neaton Ang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04864092167858940876"}},"outputId":"55a3d243-e9d6-4072-ff41-586d5a542985"},"source":["%cd '../'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1ixE_YVbTLblbUJgpPcDWfO-zlQeoNGq4/CS3244 45 Project\n"]}]},{"cell_type":"code","metadata":{"id":"Ienaxu8Y2XYw"},"source":["embeddings_index = {}\n","with open('glove.840B.300d.txt', encoding='utf-8') as f:\n","  for line in f:\n","    values = line.split(' ')\n","    word = values[0]\n","    embedding = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = embedding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Yu1QCvA2RiW"},"source":["def sent2vec(s):\n","    words = str(s).lower()\n","    words = word_tokenize(words)\n","    words = [w for w in words if not w in stop_words]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(embeddings_index[w])\n","        except:\n","            M.append(np.zeros((1, 300)))\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    return v / np.sqrt((v ** 2).sum())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P3ZtgoICRjQq"},"source":["def wrapper(train_set):\n","\n","  #Preprocessing questions\n","  train_set['question1'] = train_set['question1'].apply(preprocess)\n","  train_set['question2'] = train_set['question2'].apply(preprocess)\n","\n","  #JA\n","  train_set['common_words_ratio'] = train_set.apply(num_common_words_ratio, axis=1)\n","  train_set['common_tokens_ratio'] = train_set.apply(common_tokens_ratio_max, axis=1)\n","  train_set['fuzz_partial_ratio'] = train_set.apply(fuzz_partial_ratio, axis=1)\n","  train_set['min_longest_substring'] = train_set.apply(min_longest_substring, axis=1)\n","\n","  #Penn Han\n","  train_set[\"unique_words_count\"] = train_set.apply(unique_words_count, axis=1)\n","  train_set[\"common_token_ratio_min\"] = train_set.apply(common_token_ratio_min, axis=1)\n","  train_set[\"fuzz_ratio\"] = train_set.apply(fuzz_ratio, axis=1)\n","  train_set[\"abs_len_difference\"] = train_set.apply(abs_len_difference, axis=1)\n","\n","  #train_set[\"tfidf_matrix_q1\"] = train_set.apply(mean_tfidf_value_q1, axis=1)\n","  #train_set[\"tfidf_matrix_q2\"] = train_set.apply(mean_tfidf_value_q2, axis=1)\n","  #train_set[\"mean_idfweighted_vector_q1\"] = train_set.apply(mean_idfweighted_vector_q1, axis=1)\n","  #train_set[\"mean_idfweighted_vector_q2\"] = train_set.apply(mean_idfweighted_vector_q2, axis=1)\n","\n","  #Jeremy\n","  train_set['common_stop_words_min'] = train_set.apply(calc_common_stop_words_min, axis=1)\n","  train_set['common_nouns_min'] = train_set.apply(calc_common_nouns_min, axis=1)\n","  train_set['mean_len'] = train_set.apply(mean_len_qns, axis=1)\n","\n","  #KC\n","  train_set['common_stop_words_max'] = train_set.apply(calc_common_stop_words_max, axis=1)\n","  train_set['common_nouns_max'] = train_set.apply(calc_common_nouns_max, axis=1)\n","  train_set['ratio_len_qn'] = train_set.apply(ratio_len_qn, axis=1)\n","\n","  #YS\n","  train_set['common_word_ratio_max'] = train_set.apply(common_word_ratio_max, axis=1)\n","  train_set['common_adjectives_max'] = train_set.apply(common_adjectives_max, axis=1)\n","  train_set['fuzz_token_set_ratio'] = train_set.apply(calc_fuzz_token_set_ratio, axis=1)\n","\n","  #Neaton\n","  train_set['common_words_ratio_min'] = train_set.apply(common_words_ratio_min, axis=1)\n","  train_set['common_adjectives_min'] = train_set.apply(common_adjectives_min, axis=1)\n","  train_set['fuzz_token_sort_ratio'] = train_set.apply(fuzz_token_sort_ratio, axis=1)\n","  train_set['max_longest_substring'] = train_set.apply(max_longest_substring, axis=1)\n","\n","  question1_vectors = np.zeros((train_set.shape[0], 300))\n","  for i, q in enumerate(tqdm_notebook(train_set.question1.values)):\n","    question1_vectors[i, :] = sent2vec(q)\n","    \n","  question2_vectors  = np.zeros((train_set.shape[0], 300))\n","  for i, q in enumerate(tqdm_notebook(train_set.question2.values)):\n","      question2_vectors[i, :] = sent2vec(q)\n","    \n","  train_set['embed_cos_dist'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors), np.nan_to_num(question2_vectors))]\n","\n","\n","  return train_set\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4sJKtWjs7BeF"},"source":["train_set = pd.read_csv('features_with_word_embedding.csv', index_col=[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cMLzheE06jU"},"source":["#features = wrapper(train_set)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKVW9vr06-6J"},"source":["SEED = 42\n","TRAIN_TEST = 0.1\n","MAX_WORDS = 20000\n","MAX_SEQUENCE = 25\n","\n","Y_labels = train_set[\"is_duplicate\"]\n","X_features = train_set.drop(\"is_duplicate\", axis=1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_features, Y_labels, test_size=TRAIN_TEST, random_state=SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90zAXRQJ8uM7"},"source":["q1_test = X_test['question1']\n","q2_test = X_test['question2']\n","q1_train = X_train['question1'].astype(str)\n","q2_train = X_train['question2'].astype(str)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ecH101Igecfu"},"source":["X_test.drop(['question1', 'question2', 'qid1', 'qid2'], axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ue0SukEqdu9E"},"source":["#dic = {'question1':[\"How to overcome fear\"], \"question2\": [\"How not to be scared\"] }\n","\n","#train_set = pd.DataFrame(dic)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IyZ8JrJC1dFz"},"source":["#final_features = features.drop(['question1', 'question2'], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mvY_uBjcCWCJ"},"source":["id = X_test.reset_index()['id']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"14rFOfHCYDfL"},"source":["MAX_SEQUENCE = 25\n","questions = q1_train.tolist() + q2_train.tolist()\n","tokenizer = Tokenizer(num_words=MAX_WORDS)\n","tokenizer.fit_on_texts(questions)\n","\n","question1_token = tokenizer.texts_to_sequences(q1_test.tolist())\n","question2_token = tokenizer.texts_to_sequences(q2_test.tolist())\n","\n","q1_prepared = pad_sequences(question1_token, maxlen=MAX_SEQUENCE)\n","q2_prepared = pad_sequences(question2_token, maxlen=MAX_SEQUENCE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CDTyhYQ9YudM","executionInfo":{"status":"ok","timestamp":1637999775574,"user_tz":-480,"elapsed":25,"user":{"displayName":"Neaton Ang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04864092167858940876"}},"outputId":"0b9aaa3b-c108-4c84-9fe5-fe3c9498a1ae"},"source":["q1_prepared"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[    0,     0,     0, ...,    44,     6, 13783],\n","       [    0,     0,     0, ...,  4489,     7,   935],\n","       [    0,     0,     0, ...,    12,   851,   118],\n","       ...,\n","       [    0,     0,     0, ...,  5396,    87,   342],\n","       [  602,   113,   180, ...,     4,  1453,    59],\n","       [    0,     0,     0, ...,   235,   299,   184]], dtype=int32)"]},"metadata":{},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"A4uT3ztKDyBX"},"source":["## Final RNN Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ek1UFimferXX","executionInfo":{"status":"ok","timestamp":1637999817484,"user_tz":-480,"elapsed":41919,"user":{"displayName":"Neaton Ang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04864092167858940876"}},"outputId":"8b172dce-3e96-416c-9417-bb6e67d7a28c"},"source":["incorrects = model.predict([q1_prepared, q2_prepared, X_test], verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1264/1264 [==============================] - 39s 29ms/step\n"]}]},{"cell_type":"code","metadata":{"id":"LJq2Yv98DGhz"},"source":["incorrects[incorrects > 0.5] = 1\n","incorrects[incorrects <= 0.5] = 0\n","flattened_incorrect = incorrects.flatten()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cIcR2bx1DR8p"},"source":["y_pred = flattened_incorrect"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZBWr4wKxDWzZ"},"source":["final_df = pd.DataFrame(id)\n","final_df['Actual'] = y_test.values\n","final_df['Pred'] = y_pred.astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8KinxPMiFpfo"},"source":["wrong_class = final_df[final_df['Actual'] != final_df['Pred']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h2M5MW_EGmb5"},"source":["## Deep dive into the False Positives For Tuned Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"2oU3ZxHSElw4","executionInfo":{"status":"ok","timestamp":1637999817493,"user_tz":-480,"elapsed":34,"user":{"displayName":"Neaton Ang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04864092167858940876"}},"outputId":"d600d5a5-3a29-422e-8710-6ea996eece50"},"source":["wrong_class"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Actual</th>\n","      <th>Pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>162455</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>158538</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>257581</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>145274</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>333020</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>40391</th>\n","      <td>195572</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>40393</th>\n","      <td>366331</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>40395</th>\n","      <td>96642</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>40408</th>\n","      <td>401326</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>40415</th>\n","      <td>297995</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>6919 rows × 3 columns</p>\n","</div>"],"text/plain":["           id  Actual  Pred\n","0      162455       0     1\n","2      158538       0     1\n","10     257581       1     0\n","11     145274       0     1\n","13     333020       0     1\n","...       ...     ...   ...\n","40391  195572       0     1\n","40393  366331       0     1\n","40395   96642       1     0\n","40408  401326       1     0\n","40415  297995       0     1\n","\n","[6919 rows x 3 columns]"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"L49G9TVfGNPC"},"source":["false_positives = wrong_class[wrong_class['Actual'] == 0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"EGsQYB-eJCi-","executionInfo":{"status":"ok","timestamp":1637999817493,"user_tz":-480,"elapsed":18,"user":{"displayName":"Neaton Ang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04864092167858940876"}},"outputId":"0370abd2-9afc-4c21-bf5d-eab56a48aeb2"},"source":["false_positives"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Actual</th>\n","      <th>Pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>162455</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>158538</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>145274</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>333020</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>145652</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>40368</th>\n","      <td>77275</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>40371</th>\n","      <td>81291</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>40391</th>\n","      <td>195572</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>40393</th>\n","      <td>366331</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>40415</th>\n","      <td>297995</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3952 rows × 3 columns</p>\n","</div>"],"text/plain":["           id  Actual  Pred\n","0      162455       0     1\n","2      158538       0     1\n","11     145274       0     1\n","13     333020       0     1\n","42     145652       0     1\n","...       ...     ...   ...\n","40368   77275       0     1\n","40371   81291       0     1\n","40391  195572       0     1\n","40393  366331       0     1\n","40415  297995       0     1\n","\n","[3952 rows x 3 columns]"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"idg9hZfLGxb5"},"source":["false_positives\n","false_positives_id = false_positives['id'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G3GcfjepACao"},"source":["fp_df = q1_test.reset_index()\n","fp_df['question2'] = q2_test.reset_index()['question2']\n","fp_df['Actual'] = y_test.values\n","fp_df['Pred'] = y_pred.astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJtAeKFuIl64"},"source":["fp_df = fp_df[fp_df.id.isin(false_positives_id)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQpAbnHKHc9z"},"source":["fp_df.reset_index(inplace=True)\n","fp_df = fp_df.drop(['index'], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"yTymp-NpAEgd","executionInfo":{"status":"ok","timestamp":1637999817496,"user_tz":-480,"elapsed":20,"user":{"displayName":"Neaton Ang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04864092167858940876"}},"outputId":"1e58f0e6-7f28-4d09-810a-3435999f3378"},"source":["fp_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>question1</th>\n","      <th>question2</th>\n","      <th>Actual</th>\n","      <th>Pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>162455</td>\n","      <td>how good a phil barone saxophones</td>\n","      <td>what are phil barone saxophones</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>158538</td>\n","      <td>how do i learn and master things</td>\n","      <td>how can i learn mastering music</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>145274</td>\n","      <td>who won the second presidential debate  trump ...</td>\n","      <td>in your opinion who won  or performed better  ...</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333020</td>\n","      <td>why is the first 20 minutes usually red colour...</td>\n","      <td>why is the first 20 minutes usually red colour...</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>145652</td>\n","      <td>how do i get rid of my addiction to facebook</td>\n","      <td>what is the best way to get rid of addictions ...</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3947</th>\n","      <td>77275</td>\n","      <td>in a world where everyone goes around naked  h...</td>\n","      <td>what if everyone in the world yelled at the sa...</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3948</th>\n","      <td>81291</td>\n","      <td>what are examples of long term goals</td>\n","      <td>what are some examples of long term and short ...</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3949</th>\n","      <td>195572</td>\n","      <td>how do i turn off 2 step verification on my gm...</td>\n","      <td>how do i recover my gmail account password wit...</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3950</th>\n","      <td>366331</td>\n","      <td>when will avicii release his new album</td>\n","      <td>will avicii release his new album in 2016</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3951</th>\n","      <td>297995</td>\n","      <td>how do i get online advice</td>\n","      <td>where can i get online advice</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3952 rows × 5 columns</p>\n","</div>"],"text/plain":["          id                                          question1  ... Actual  Pred\n","0     162455                  how good a phil barone saxophones  ...      0     1\n","1     158538                   how do i learn and master things  ...      0     1\n","2     145274  who won the second presidential debate  trump ...  ...      0     1\n","3     333020  why is the first 20 minutes usually red colour...  ...      0     1\n","4     145652       how do i get rid of my addiction to facebook  ...      0     1\n","...      ...                                                ...  ...    ...   ...\n","3947   77275  in a world where everyone goes around naked  h...  ...      0     1\n","3948   81291               what are examples of long term goals  ...      0     1\n","3949  195572  how do i turn off 2 step verification on my gm...  ...      0     1\n","3950  366331             when will avicii release his new album  ...      0     1\n","3951  297995                         how do i get online advice  ...      0     1\n","\n","[3952 rows x 5 columns]"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"6Uf0YVRBKdQU","colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"status":"ok","timestamp":1637999817496,"user_tz":-480,"elapsed":19,"user":{"displayName":"Neaton Ang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04864092167858940876"}},"outputId":"4a8708c0-e9bf-4403-f9dc-addf9c6b734f"},"source":["fp_df.loc[0:3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>question1</th>\n","      <th>question2</th>\n","      <th>Actual</th>\n","      <th>Pred</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>162455</td>\n","      <td>how good a phil barone saxophones</td>\n","      <td>what are phil barone saxophones</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>158538</td>\n","      <td>how do i learn and master things</td>\n","      <td>how can i learn mastering music</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>145274</td>\n","      <td>who won the second presidential debate  trump ...</td>\n","      <td>in your opinion who won  or performed better  ...</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>333020</td>\n","      <td>why is the first 20 minutes usually red colour...</td>\n","      <td>why is the first 20 minutes usually red colour...</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id                                          question1  ... Actual  Pred\n","0  162455                  how good a phil barone saxophones  ...      0     1\n","1  158538                   how do i learn and master things  ...      0     1\n","2  145274  who won the second presidential debate  trump ...  ...      0     1\n","3  333020  why is the first 20 minutes usually red colour...  ...      0     1\n","\n","[4 rows x 5 columns]"]},"metadata":{},"execution_count":64}]}]}